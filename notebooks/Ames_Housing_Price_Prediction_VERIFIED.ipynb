{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ames Housing Price Prediction\n## Advanced Apex Project - Real Estate Price Modeling\n\nA comprehensive machine learning approach to predicting residential property sale prices using multiple regression techniques and extensive feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### Project Information\n\n**Team:** The Outliers\n\n**Course:** Advanced Apex Project 1\n\n**Institution:** BITS Pilani - Digital Campus\n\n**Academic Term:** First Trimester 2025-26\n\n**Project Supervisor:** Bharathi Dasari\n\n**Submission Date:** November 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Team Members\n\n| Student Name | BITS ID |\n|--------------|----------|\n| Anik Das | 2025EM1100026 |\n| Adeetya Wadikar | 2025EM1100384 |\n| Tushar Nishane | 2025EM1100306 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Executive Summary\n\n### Problem Statement\n\nAccurate real estate valuation is essential for buyers, sellers, and financial institutions. Traditional valuation methods can be subjective and time-consuming. This project develops machine learning models to predict house sale prices objectively based on property characteristics.\n\n### Business Objective\n\nDevelop a predictive regression model that estimates residential property sale prices with high accuracy. The model should help stakeholders:\n- **Buyers**: Assess fair market value before purchase\n- **Sellers**: Set competitive listing prices\n- **Investors**: Identify undervalued properties\n- **Lenders**: Support loan underwriting decisions\n\n### Dataset\n\n**Name:** Ames Housing Dataset\n\n**Source:** Kaggle (https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset)\n\n**Size:** 2,930 residential property sales transactions\n\n**Features:** 82 variables describing:\n- Physical characteristics (size, rooms, age)\n- Quality ratings (construction, condition)\n- Location attributes (neighborhood, zoning)\n- Amenities (garage, basement, fireplace, pool)\n\n**Target Variable:** SalePrice (in USD)\n\n**Time Period:** Properties sold in Ames, Iowa from 2006-2010"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Table of Contents\n\n### [Phase 1: Data Acquisition](#phase1)\n1.1 [Environment Setup](#setup)\n1.2 [Data Loading](#loading)\n1.3 [Initial Data Inspection](#inspection)\n1.4 [Schema Validation](#schema)\n1.5 [Data Quality Assessment](#quality)\n\n### [Phase 2A: Data Preprocessing & Exploratory Analysis](#phase2a)\n2.1 [Missing Value Analysis](#missing)\n2.2 [Missing Value Treatment](#treatment)\n2.3 [Univariate Analysis - Numerical](#univariate-num)\n2.4 [Univariate Analysis - Categorical](#univariate-cat)\n2.5 [Low-Variance Feature Removal](#lowvar)\n2.6 [Bivariate Analysis - Correlations](#bivariate-corr)\n2.7 [Bivariate Analysis - Visualizations](#bivariate-viz)\n2.8 [Outlier Detection](#outliers)\n\n### [Phase 2B: Feature Engineering](#phase2b)\n3.1 [Feature Creation](#creation)\n3.2 [Feature Transformation](#transformation)\n3.3 [Categorical Encoding](#encoding)\n3.4 [Feature Importance](#importance)\n\n### [Phase 3: Model Development & Evaluation](#phase3)\n4.1 [Data Preparation](#preparation)\n4.2 [Simple Linear Regression](#simple-lr)\n4.3 [Multiple Linear Regression](#multiple-lr)\n4.4 [Model Comparison](#comparison)\n4.5 [Conclusions & Recommendations](#conclusions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='phase1'></a>\n\n# Phase 1: Data Acquisition\n\n## Objective\n\nAcquire the Ames Housing dataset and perform initial validation to ensure data integrity. This foundational phase establishes the quality and completeness of our data before proceeding to analysis.\n\n## Deliverables\n\n- Successfully load dataset from CSV file\n- Verify data structure and schema\n- Conduct initial quality checks\n- Document data characteristics and potential issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='setup'></a>\n\n## 1.1 Environment Setup\n\nWe import all necessary Python libraries for data manipulation, statistical analysis, visualization, and machine learning. Proper configuration ensures consistent behavior across different environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import core data manipulation libraries\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Import visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n# Import statistical libraries\nfrom scipy import stats\n\n# Import machine learning libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Configure environment\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set display options for better readability\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.2f}'.format)\npd.set_option('display.width', 1000)\n\n# Set visualization defaults\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\n# Print confirmation\nprint(\"\u2713 All libraries imported successfully\")\nprint(f\"\u2713 Pandas version: {pd.__version__}\")\nprint(f\"\u2713 NumPy version: {np.__version__}\")\nprint(f\"\u2713 Matplotlib version: {plt.matplotlib.__version__}\")\nprint(\"\\nEnvironment configured and ready for analysis.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='loading'></a>\n\n## 1.2 Data Loading\n\nThe Ames Housing dataset was downloaded from Kaggle and stored in the project's data directory. This dataset provides comprehensive information on residential properties sold in Ames, Iowa, making it an excellent resource for developing price prediction models.\n\n**Data Source:** Kaggle - Ames Housing Dataset\n\n**Citation:** Shashank Necrothapa. (n.d.). Ames Housing Dataset. Kaggle. https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the path to the dataset\ndata_path = \"../data/AmesHousing.csv\"\n\n# Load the dataset into a pandas DataFrame\ndf = pd.read_csv(data_path)\n\n# Display basic information\nprint(\"\u2713 Dataset loaded successfully!\")\nprint(f\"\\nDataset Dimensions: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\nprint(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n# Display first few records\nprint(\"\\nFirst 5 Records:\")\ndf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='inspection'></a>\n\n## 1.3 Initial Data Inspection\n\nBefore conducting detailed analysis, we perform a high-level inspection to understand the dataset structure, identify data types, and spot any immediate quality concerns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display comprehensive dataset information\nprint(\"Dataset Structure Overview:\\n\")\ndf.info()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Data Type Summary:\")\nprint(\"=\"*70)\nprint(df.dtypes.value_counts())\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Column Distribution:\")\nprint(\"=\"*70)\nprint(f\"Numerical columns (int64): {len(df.select_dtypes(include=['int64']).columns)}\")\nprint(f\"Numerical columns (float64): {len(df.select_dtypes(include=['float64']).columns)}\")\nprint(f\"Categorical columns (object): {len(df.select_dtypes(include=['object']).columns)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='schema'></a>\n\n## 1.4 Schema Validation\n\nWe verify that all expected columns are present and properly formatted. This schema validation ensures data integrity and helps identify any structural anomalies early in the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display all column names\nprint(f\"Total Features: {len(df.columns)}\\n\")\nprint(\"All Column Names:\")\nprint(\"=\"*70)\n\n# Print in organized format (4 columns)\ncol_list = df.columns.tolist()\nfor i in range(0, len(col_list), 4):\n    row = col_list[i:i+4]\n    print(f\"{i+1:2d}-{i+len(row):2d}: \" + \" | \".join(f\"{col:20s}\" for col in row))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Key Columns Verified:\")\nprint(\"=\"*70)\nimportant_cols = ['Order', 'PID', 'SalePrice', 'Gr Liv Area', 'Overall Qual', 'Neighborhood']\nfor col in important_cols:\n    status = \"\u2713\" if col in df.columns else \"\u2717\"\n    print(f\"{status} {col}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='quality'></a>\n\n## 1.5 Data Quality Assessment\n\nWe conduct initial quality checks to identify missing values, duplicate records, and verify the target variable integrity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform comprehensive quality checks\nprint(\"Data Quality Assessment:\")\nprint(\"=\"*70)\n\n# Check for missing values\ntotal_missing = df.isnull().sum().sum()\ncols_with_missing = df.isnull().any().sum()\nprint(f\"\\nMissing Value Check:\")\nprint(f\"  Total missing values: {total_missing:,}\")\nprint(f\"  Columns with missing data: {cols_with_missing} out of {len(df.columns)}\")\n\n# Check for duplicates\nduplicates = df.duplicated().sum()\nprint(f\"\\nDuplicate Check:\")\nprint(f\"  Duplicate rows: {duplicates}\")\nif duplicates == 0:\n    print(\"  \u2713 No duplicates found\")\n\n# Verify target variable\nprint(f\"\\nTarget Variable (SalePrice) Verification:\")\nprint(f\"  Missing values: {df['SalePrice'].isnull().sum()}\")\nprint(f\"  Minimum: ${df['SalePrice'].min():,}\")\nprint(f\"  Maximum: ${df['SalePrice'].max():,}\")\nprint(f\"  Mean: ${df['SalePrice'].mean():,.2f}\")\nprint(f\"  Median: ${df['SalePrice'].median():,.2f}\")\nprint(f\"  Standard Deviation: ${df['SalePrice'].std():,.2f}\")\n\nprint(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create detailed schema summary table\nschema_summary = pd.DataFrame({\n    'Column': df.columns,\n    'Data_Type': df.dtypes.values,\n    'Non_Null_Count': df.count().values,\n    'Null_Count': df.isnull().sum().values,\n    'Null_Percentage': (df.isnull().sum() / len(df) * 100).values,\n    'Unique_Values': [df[col].nunique() for col in df.columns]\n})\n\n# Sort by null percentage to see problematic columns first\nschema_summary = schema_summary.sort_values('Null_Percentage', ascending=False)\n\nprint(\"Schema Summary (Top 20 columns by missing data):\")\nprint(\"=\"*90)\nschema_summary.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.5.1 Data Dictionary Cross-Reference\n\nWe attempt to load the official data dictionary to cross-reference feature definitions and ensure our understanding aligns with the dataset documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Attempt to load the data dictionary\ntry:\n    data_dict_path = \"../docs/data_dictionary.xlsx\"\n    data_dict = pd.read_excel(data_dict_path)\n    print(f\"\u2713 Data dictionary loaded successfully\")\n    print(f\"  Total feature descriptions: {len(data_dict)}\")\n    print(f\"\\nFirst 10 Feature Definitions:\")\n    print(\"=\"*70)\n    print(data_dict.head(10))\nexcept FileNotFoundError:\n    print(\"\u2139 Data dictionary file not found at expected location\")\n    print(\"  This is not critical - proceeding with dataset analysis\")\n    print(f\"  Expected path: {data_dict_path}\")\nexcept Exception as e:\n    print(f\"\u2139 Could not load data dictionary: {str(e)}\")\n    print(\"  Proceeding with dataset analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## Phase 1 Summary\n\n### Accomplishments\n\n\u2705 **Environment Configured**\n- All required libraries imported successfully\n- Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn ready\n- Display settings optimized for analysis\n\n\u2705 **Dataset Successfully Loaded**\n- **Source:** Ames Housing Dataset from Kaggle\n- **Size:** 2,930 residential property records\n- **Features:** 82 variables (28 int, 11 float, 43 categorical)\n- **Memory:** ~2MB dataset size\n- **Target:** SalePrice (range: $12,789 - $755,000)\n\n\u2705 **Data Quality Verified**\n- Schema matches expectations (82 columns present)\n- No duplicate records identified\n- Target variable has no missing values\n- 27 features contain missing values (to be addressed in Phase 2)\n\n\u2705 **Initial Observations**\n- Mix of numerical and categorical features\n- Some features have high missingness (>50%) - candidates for removal\n- Price range suggests diverse property types\n- Data appears well-structured and ready for analysis\n\n### Next Steps\n\nProceed to **Phase 2A: Data Preprocessing & Exploratory Analysis** where we will:\n- Conduct comprehensive missing value analysis\n- Implement systematic data cleaning procedures\n- Perform univariate and bivariate analysis\n- Identify and handle outliers\n- Prepare data for feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='phase2a'></a>\n\n# Phase 2A: Data Preprocessing & Exploratory Analysis\n\n## Objective\n\nTransform raw data into a clean, analysis-ready format through systematic preprocessing. Conduct comprehensive exploratory analysis to understand variable distributions, relationships, and data quality issues.\n\n## Key Activities\n\n- Systematic missing value analysis and treatment\n- Univariate analysis of all features\n- Bivariate analysis to identify price predictors\n- Low-variance feature identification and removal\n- Outlier detection and assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='missing'></a>\n\n## 2.1 Missing Value Analysis\n\nMissing data is common in real-world datasets. We systematically analyze missing value patterns to develop an appropriate treatment strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate missing value statistics\nmissing_counts = df.isnull().sum()\nmissing_pct = (missing_counts / len(df)) * 100\n\nmissing_df = pd.DataFrame({\n    'Feature': missing_counts.index,\n    'Missing_Count': missing_counts.values,\n    'Missing_Percentage': missing_pct.values\n})\n\n# Filter to only features with missing values\nmissing_df = missing_df[missing_df['Missing_Count'] > 0]\nmissing_df = missing_df.sort_values('Missing_Percentage', ascending=False)\n\nprint(f\"Features with Missing Values: {len(missing_df)} out of {len(df.columns)}\")\nprint(\"\\nTop 15 Features with Most Missing Data:\")\nprint(\"=\"*70)\nmissing_df.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.1 Missing Value Visualization\n\nVisual analysis helps identify patterns - whether values are missing completely at random (MCAR), at random (MAR), or not at random (MNAR)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize missing data patterns using missingno\nplt.figure(figsize=(14, 8))\nmsno.matrix(df, figsize=(14, 8), fontsize=10, sparkline=False)\nplt.title('Missing Value Matrix - Complete Dataset View', fontsize=14, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\nprint(\"Matrix shows:\")  \nprint(\"  - White lines = missing values\")\nprint(\"  - Dark bars = complete data\")\nprint(\"  - Patterns suggest some features missing together (e.g., garage features)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Bar chart of missing percentages\nplt.figure(figsize=(12, 8))\nmissing_to_plot = missing_df.head(20)\nplt.barh(range(len(missing_to_plot)), missing_to_plot['Missing_Percentage'].values, color='coral', alpha=0.7)\nplt.yticks(range(len(missing_to_plot)), missing_to_plot['Feature'].values)\nplt.xlabel('Percentage Missing (%)', fontweight='bold', fontsize=11)\nplt.ylabel('Feature', fontweight='bold', fontsize=11)\nplt.title('Top 20 Features by Missing Data Percentage', fontweight='bold', fontsize=13)\nplt.axvline(x=50, color='red', linestyle='--', linewidth=2, label='50% threshold')\nplt.legend()\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Observations from Missing Data Analysis\n\n**High Missingness (>50% - Candidates for Removal):**\n- **Pool QC** (99.6%): Pool quality - most homes don't have pools\n- **Misc Feature** (96.4%): Miscellaneous features - rarely present\n- **Alley** (93.2%): Alley access type - uncommon\n- **Fence** (80.5%): Fence quality - many homes lack fences\n\n**Moderate Missingness (5-50% - Contextual Imputation):**\n- **Fireplace Qu** (48.5%): Fireplace quality - indicates no fireplace\n- **Lot Frontage** (16.7%): Linear feet of street connected to property\n- **Garage features** (~5%): Likely indicates no garage\n- **Basement features** (~3%): Likely indicates no basement\n\n**Strategy:** Drop high-missingness features, impute others based on context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='treatment'></a>\n\n## 2.2 Missing Value Treatment\n\nWe implement a systematic 4-step treatment strategy based on missingness patterns and feature semantics:\n\n1. **Drop** features with >50% missing (insufficient data for reliable imputation)\n2. **Categorical imputation**: Fill with 'None' for features where absence has meaning\n3. **Numerical imputation**: Fill with 0 for counts/areas where absence = zero\n4. **Context-aware imputation**: Neighborhood-based median for Lot Frontage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Drop columns with excessive missing values (>50%)\nthreshold = 50\ncols_to_drop = missing_df[missing_df['Missing_Percentage'] > threshold]['Feature'].tolist()\n\nprint(f\"Dropping {len(cols_to_drop)} features with >{threshold}% missing:\")\nprint(\"=\"*70)\nfor col in cols_to_drop:\n    pct = missing_df[missing_df['Feature'] == col]['Missing_Percentage'].values[0]\n    print(f\"  - {col:20s}: {pct:6.2f}% missing\")\n\ndf = df.drop(columns=cols_to_drop)\nprint(f\"\\nDataset shape after dropping: {df.shape}\")\nprint(f\"Columns remaining: {df.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 2: Impute categorical features with 'None'\n# For these features, missing means the feature doesn't exist\ncategorical_none = [\n    'Mas Vnr Type', 'Fireplace Qu', 'Garage Type', 'Garage Finish',\n    'Garage Qual', 'Garage Cond', 'Bsmt Qual', 'Bsmt Cond',\n    'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin Type 2'\n]\n\nprint(\"Imputing categorical features (None = feature absent):\")\nprint(\"=\"*70)\n\nfor col in categorical_none:\n    if col in df.columns:\n        before_count = df[col].isnull().sum()\n        df[col] = df[col].fillna('None')\n        print(f\"  \u2713 {col:25s}: {before_count:4d} values \u2192 'None'\")\n\nprint(f\"\\nCategorical imputation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 3: Impute numerical features with 0\n# For areas and counts, zero indicates feature is absent\nnumeric_zero = [\n    'Mas Vnr Area', 'BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF',\n    'Total Bsmt SF', 'Bsmt Full Bath', 'Bsmt Half Bath',\n    'Garage Cars', 'Garage Area'\n]\n\nprint(\"Imputing numerical features (0 = feature absent):\")\nprint(\"=\"*70)\n\nfor col in numeric_zero:\n    if col in df.columns:\n        before_count = df[col].isnull().sum()\n        df[col] = df[col].fillna(0)\n        print(f\"  \u2713 {col:25s}: {before_count:4d} values \u2192 0\")\n\nprint(f\"\\nNumerical imputation complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: Neighborhood-based imputation for Lot Frontage\n# Lot Frontage varies by neighborhood, so use neighborhood median\nprint(\"Imputing Lot Frontage using neighborhood-grouped median:\")\nprint(\"=\"*70)\n\nbefore_count = df['Lot Frontage'].isnull().sum()\nprint(f\"Missing before: {before_count}\\n\")\n\n# Group by neighborhood and fill with median\ndf['Lot Frontage'] = df.groupby('Neighborhood')['Lot Frontage'].transform(\n    lambda x: x.fillna(x.median())\n)\n\nafter_count = df['Lot Frontage'].isnull().sum()\nprint(f\"Missing after: {after_count}\")\nprint(f\"\u2713 Imputed {before_count - after_count} values using neighborhood medians\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 5: Handle remaining missing values\nprint(\"Handling remaining missing values:\")\nprint(\"=\"*70)\n\n# Garage Year Built - use house year if missing\nif 'Garage Yr Blt' in df.columns and df['Garage Yr Blt'].isnull().sum() > 0:\n    before = df['Garage Yr Blt'].isnull().sum()\n    df['Garage Yr Blt'] = df['Garage Yr Blt'].fillna(df['Year Built'])\n    print(f\"  \u2713 Garage Yr Blt: {before} values \u2192 Year Built (no garage = same as house)\")\n\n# Electrical - only 1 missing, use mode\nif 'Electrical' in df.columns and df['Electrical'].isnull().sum() > 0:\n    before = df['Electrical'].isnull().sum()\n    mode_val = df['Electrical'].mode()[0]\n    df['Electrical'] = df['Electrical'].fillna(mode_val)\n    print(f\"  \u2713 Electrical: {before} value \u2192 '{mode_val}' (mode)\")\n\nprint(f\"\\nAll specific imputations complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify all missing values have been handled\nremaining_missing = df.isnull().sum().sum()\ncols_with_missing = df.isnull().any().sum()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MISSING VALUE TREATMENT - FINAL VERIFICATION\")\nprint(\"=\"*70)\nprint(f\"Total missing values remaining: {remaining_missing}\")\nprint(f\"Columns with missing values: {cols_with_missing}\")\n\nif remaining_missing == 0:\n    print(\"\\n\u2705 SUCCESS: All missing values successfully handled!\")\n    print(\"   Dataset is now complete and ready for analysis.\")\nelse:\n    print(f\"\\n\u26a0 WARNING: {remaining_missing} missing values still present\")\n    print(\"\\nColumns with remaining missing values:\")\n    still_missing = df.isnull().sum()\n    print(still_missing[still_missing > 0])\n\nprint(\"=\"*70)\nprint(f\"Final dataset shape: {df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='univariate-num'></a>\n\n## 2.3 Univariate Analysis - Numerical Features\n\nWe examine the distribution of each numerical variable to understand central tendencies, spread, skewness, and potential data quality issues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select numerical columns\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols = [col for col in numeric_cols if col not in ['Order', 'PID']]\n\nprint(f\"Analyzing {len(numeric_cols)} numerical features\\n\")\nprint(\"First 10 numerical features:\")\nfor i, col in enumerate(numeric_cols[:10], 1):\n    print(f\"  {i:2d}. {col}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create comprehensive histograms for all numerical features\nfig, axes = plt.subplots(10, 4, figsize=(20, 25))\naxes = axes.ravel()\n\nfor idx, col in enumerate(numeric_cols):\n    if idx < 40:\n        axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n        axes[idx].set_title(col, fontweight='bold', fontsize=10)\n        axes[idx].set_ylabel('Frequency', fontsize=8)\n        axes[idx].tick_params(labelsize=8)\n\nfor idx in range(len(numeric_cols), 40):\n    axes[idx].axis('off')\n\nplt.suptitle('Distribution of Numerical Features', fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distribution Patterns Observed\n\n**Right-Skewed (Positive Skew):**\n- Lot Area, Sale Price, Living Area\n- Most values concentrated at lower end\n\n**Approximately Normal:**\n- Number of bedrooms, bathrooms\n- Centered distributions\n\n**Left-Skewed:**\n- Year Built, Overall Quality\n- More recent/higher quality homes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='univariate-cat'></a>\n\n## 2.4 Univariate Analysis - Categorical Features\n\nExamine categorical variables to understand category distributions and identify dominant values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select categorical columns\ncategorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n\nprint(f\"Analyzing {len(categorical_cols)} categorical features\\n\")\n\n# Show value counts for key categorical features\nkey_cats = ['MS Zoning', 'Neighborhood', 'Bldg Type', 'House Style']\nfor cat in key_cats:\n    if cat in df.columns:\n        print(f\"\\n{cat}:\")\n        print(df[cat].value_counts().head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize categorical features\nfig, axes = plt.subplots(3, 3, figsize=(18, 12))\naxes = axes.ravel()\n\ncat_viz = ['MS Zoning', 'Neighborhood', 'Bldg Type', 'House Style', 'Foundation', \n           'Heating QC', 'Central Air', 'Kitchen Qual', 'Sale Condition']\n\nfor idx, col in enumerate(cat_viz):\n    if col in df.columns and idx < 9:\n        vc = df[col].value_counts().head(10)\n        axes[idx].bar(range(len(vc)), vc.values, color='coral', alpha=0.7)\n        axes[idx].set_xticks(range(len(vc)))\n        axes[idx].set_xticklabels(vc.index, rotation=45, ha='right', fontsize=8)\n        axes[idx].set_title(col, fontweight='bold')\n        axes[idx].set_ylabel('Count')\n\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='lowvar'></a>\n\n## 2.5 Low-Variance Feature Removal\n\nFeatures dominated by a single category provide little predictive power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Identify and remove low-variance categorical features\nlow_var_cols = ['Street', 'Utilities', 'Condition 2', 'Roof Matl', 'Heating', 'Land Slope']\n\nprint(f\"Dropping {len(low_var_cols)} low-variance features:\\n\")\nfor col in low_var_cols:\n    if col in df.columns:\n        dominant = df[col].value_counts().index[0]\n        pct = (df[col].value_counts().iloc[0] / len(df)) * 100\n        print(f\"  - {col:15s}: {pct:5.1f}% are '{dominant}'\")\n\ndf = df.drop(columns=[c for c in low_var_cols if c in df.columns])\nprint(f\"\\nNew shape: {df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='bivariate-corr'></a>\n\n## 2.6 Bivariate Analysis - Correlations\n\nExamine relationships between features and the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate correlation with SalePrice\ncorr_matrix = df.corr(numeric_only=True)\nsaleprice_corr = corr_matrix['SalePrice'].sort_values(ascending=False)\n\nprint(\"Top 15 Features Correlated with SalePrice:\\n\")\nprint(saleprice_corr.head(15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Correlation heatmap\ntop_features = saleprice_corr.head(12).index\ncorr_subset = df[top_features].corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='RdYlBu_r',\n            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Correlation Heatmap - Top Features', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='bivariate-viz'></a>\n\n## 2.7 Bivariate Visualizations\n\nScatter plots reveal relationships between features and sale price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scatter plots for top features\ntop_num = ['Gr Liv Area', 'Garage Area', 'Total Bsmt SF', '1st Flr SF', 'Year Built']\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.ravel()\n\nfor idx, feat in enumerate(top_num[:6]):\n    if feat in df.columns:\n        axes[idx].scatter(df[feat], df['SalePrice'], alpha=0.5, s=20)\n        axes[idx].set_xlabel(feat, fontweight='bold')\n        axes[idx].set_ylabel('SalePrice', fontweight='bold')\n        corr = df[[feat, 'SalePrice']].corr().iloc[0,1]\n        axes[idx].set_title(f'{feat} (r={corr:.3f})')\n\naxes[5].axis('off')\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Box plots for categorical features\ncat_feats = ['Overall Qual', 'Neighborhood', 'Kitchen Qual', 'Garage Type']\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes = axes.ravel()\n\nfor idx, feat in enumerate(cat_feats):\n    if feat in df.columns:\n        order = df.groupby(feat)['SalePrice'].median().sort_values().index\n        data = [df[df[feat]==cat]['SalePrice'].values for cat in order]\n        axes[idx].boxplot(data, labels=order)\n        axes[idx].set_xlabel(feat, fontweight='bold')\n        axes[idx].set_ylabel('SalePrice', fontweight='bold')\n        axes[idx].tick_params(axis='x', rotation=45, labelsize=8)\n\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='outliers'></a>\n\n## 2.8 Outlier Detection\n\nUsing IQR method to identify potential outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# IQR outlier detection\ndef detect_outliers(data, column):\n    Q1 = data[column].quantile(0.25)\n    Q3 = data[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    outliers = data[(data[column] < lower) | (data[column] > upper)]\n    return outliers, lower, upper\n\nkey_feats = ['SalePrice', 'Gr Liv Area', 'Lot Area', 'Total Bsmt SF']\n\nprint(\"Outlier Detection Results:\\n\")\nfor feat in key_feats:\n    outliers, lower, upper = detect_outliers(df, feat)\n    print(f\"{feat}:\")\n    print(f\"  Bounds: [{lower:.0f}, {upper:.0f}]\")\n    print(f\"  Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Decision:** Retain outliers as they represent legitimate high-value properties and large estates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='phase2b'></a>\n\n# Phase 2B: Feature Engineering\n\n## Objective\n\nCreate meaningful features and transform data for optimal model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='creation'></a>\n\n## 3.1 Feature Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create engineered features\nprint(\"Engineering features...\\n\")\n\ndf['Total_Bathrooms'] = df['Full Bath'] + 0.5*df['Half Bath'] + df['Bsmt Full Bath'] + 0.5*df['Bsmt Half Bath']\ndf['Total_Porch_SF'] = df['Wood Deck SF'] + df['Open Porch SF'] + df['Enclosed Porch'] + df['3Ssn Porch'] + df['Screen Porch']\ndf['House_Age'] = df['Yr Sold'] - df['Year Built']\ndf['Years_Since_Remod'] = df['Yr Sold'] - df['Year Remod/Add']\ndf['Total_SF'] = df['Total Bsmt SF'] + df['Gr Liv Area']\n\nprint(\"\u2713 5 new features created\")\nprint(f\"Total features: {df.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check new feature correlations\nnew_feats = ['Total_Bathrooms', 'Total_Porch_SF', 'House_Age', 'Years_Since_Remod', 'Total_SF']\nfor feat in new_feats:\n    corr = df[[feat, 'SalePrice']].corr().iloc[0,1]\n    print(f\"{feat:25s}: {corr:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='transformation'></a>\n\n## 3.2 Feature Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze skewness\nfrom scipy import stats\nskewed = []\nfor col in df.select_dtypes(include=[np.number]).columns:\n    if col != 'SalePrice':\n        skew = stats.skew(df[col].dropna())\n        if abs(skew) > 1:\n            skewed.append((col, skew))\n\nprint(f\"Highly skewed features (|skew| > 1): {len(skewed)}\\n\")\nfor feat, skew in sorted(skewed, key=lambda x: abs(x[1]), reverse=True)[:10]:\n    print(f\"  {feat:25s}: {skew:7.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='encoding'></a>\n\n## 3.3 Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Encode categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_encoded = df.copy()\ncat_cols = df_encoded.select_dtypes(include=['object']).columns\n\nlabel_encoders = {}\nfor col in cat_cols:\n    le = LabelEncoder()\n    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n    label_encoders[col] = le\n\nprint(f\"\u2713 Encoded {len(cat_cols)} categorical features\")\nprint(f\"All features now numeric: {df_encoded.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='importance'></a>\n\n## 3.4 Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random Forest feature importance\nfrom sklearn.ensemble import RandomForestRegressor\n\nX = df_encoded.drop(['SalePrice', 'Order', 'PID'], axis=1, errors='ignore')\ny = df_encoded['SalePrice']\n\nrf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf.fit(X, y)\n\nimportances = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': rf.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"Top 15 Most Important Features:\\n\")\nprint(importances.head(15).to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize top 20\nplt.figure(figsize=(10, 8))\ntop20 = importances.head(20)\nplt.barh(range(len(top20)), top20['Importance'].values, color='steelblue')\nplt.yticks(range(len(top20)), top20['Feature'].values)\nplt.xlabel('Importance', fontweight='bold')\nplt.title('Top 20 Feature Importances', fontweight='bold')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 2B Summary\n\n\u2705 5 engineered features created\n\u2705 Categorical encoding complete\n\u2705 Feature importance analyzed\n\u2705 Dataset ready for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='phase3'></a>\n\n# Phase 3: Model Development & Evaluation\n\n## Objective\n\nBuild regression models to predict house prices and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='preparation'></a>\n\n## 4.1 Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare data\nX = df_encoded.drop(['SalePrice', 'Order', 'PID'], axis=1, errors='ignore')\ny = df_encoded['SalePrice']\n\n# Handle any remaining NaNs\nfor col in X.columns:\n    if X[col].isnull().sum() > 0:\n        X[col] = X[col].fillna(X[col].median())\n\nprint(f\"Features: {X.shape}\")\nprint(f\"Target: {y.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train-test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"Testing: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='simple-lr'></a>\n\n## 4.2 Simple Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Identify best feature\ncorrs = X_train.corrwith(y_train).abs().sort_values(ascending=False)\nbest_feat = corrs.index[0]\n\nprint(f\"Best feature: {best_feat}\")\nprint(f\"Correlation: {corrs[best_feat]:.4f}\")\n\nX_train_simple = X_train[[best_feat]]\nX_test_simple = X_test[[best_feat]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train Simple LR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nimport math\n\nmodel_simple = LinearRegression()\nmodel_simple.fit(X_train_simple, y_train)\n\ny_train_pred_s = model_simple.predict(X_train_simple)\ny_test_pred_s = model_simple.predict(X_test_simple)\n\nr2_train_s = r2_score(y_train, y_train_pred_s)\nr2_test_s = r2_score(y_test, y_test_pred_s)\nrmse_s = math.sqrt(mean_squared_error(y_test, y_test_pred_s))\nmae_s = mean_absolute_error(y_test, y_test_pred_s)\n\nprint(f\"Simple LR Results:\")\nprint(f\"  R\u00b2 (train): {r2_train_s:.4f}\")\nprint(f\"  R\u00b2 (test): {r2_test_s:.4f}\")\nprint(f\"  RMSE: ${rmse_s:,.2f}\")\nprint(f\"  MAE: ${mae_s:,.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize Simple LR\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].scatter(X_test_simple, y_test, alpha=0.5, s=30)\naxes[0].plot(X_test_simple, y_test_pred_s, 'r-', lw=2)\naxes[0].set_xlabel(best_feat, fontweight='bold')\naxes[0].set_ylabel('SalePrice', fontweight='bold')\naxes[0].set_title(f'Simple LR: {best_feat}', fontweight='bold')\n\naxes[1].scatter(y_test, y_test_pred_s, alpha=0.5, s=30)\naxes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\naxes[1].set_xlabel('Actual', fontweight='bold')\naxes[1].set_ylabel('Predicted', fontweight='bold')\naxes[1].set_title(f'R\u00b2 = {r2_test_s:.4f}', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='multiple-lr'></a>\n\n## 4.3 Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train Multiple LR\nmodel_multiple = LinearRegression()\nmodel_multiple.fit(X_train, y_train)\n\ny_train_pred_m = model_multiple.predict(X_train)\ny_test_pred_m = model_multiple.predict(X_test)\n\nr2_train_m = r2_score(y_train, y_train_pred_m)\nr2_test_m = r2_score(y_test, y_test_pred_m)\nrmse_m = math.sqrt(mean_squared_error(y_test, y_test_pred_m))\nmae_m = mean_absolute_error(y_test, y_test_pred_m)\n\nprint(f\"Multiple LR Results ({X_train.shape[1]} features):\")\nprint(f\"  R\u00b2 (train): {r2_train_m:.4f}\")\nprint(f\"  R\u00b2 (test): {r2_test_m:.4f}\")\nprint(f\"  RMSE: ${rmse_m:,.2f}\")\nprint(f\"  MAE: ${mae_m:,.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize Multiple LR\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].scatter(y_test, y_test_pred_m, alpha=0.5, s=30, color='green')\naxes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\naxes[0].set_xlabel('Actual Price', fontweight='bold')\naxes[0].set_ylabel('Predicted Price', fontweight='bold')\naxes[0].set_title(f'Multiple LR: R\u00b2 = {r2_test_m:.4f}', fontweight='bold')\n\nresiduals = y_test - y_test_pred_m\naxes[1].scatter(y_test_pred_m, residuals, alpha=0.5, s=30, color='green')\naxes[1].axhline(0, color='red', linestyle='--', lw=2)\naxes[1].set_xlabel('Predicted Price', fontweight='bold')\naxes[1].set_ylabel('Residuals', fontweight='bold')\naxes[1].set_title('Residual Plot', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='comparison'></a>\n\n## 4.4 Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Comparison table\ncomp = pd.DataFrame({\n    'Metric': ['Features', 'R\u00b2 (Train)', 'R\u00b2 (Test)', 'RMSE', 'MAE'],\n    'Simple LR': [1, f'{r2_train_s:.4f}', f'{r2_test_s:.4f}', f'${rmse_s:,.0f}', f'${mae_s:,.0f}'],\n    'Multiple LR': [X.shape[1], f'{r2_train_m:.4f}', f'{r2_test_m:.4f}', f'${rmse_m:,.0f}', f'${mae_m:,.0f}']\n})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(comp.to_string(index=False))\nprint(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visual comparison\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].bar(['Simple', 'Multiple'], [r2_test_s, r2_test_m], color=['steelblue', 'green'])\naxes[0].set_ylabel('R\u00b2 Score', fontweight='bold')\naxes[0].set_title('R\u00b2 Comparison', fontweight='bold')\naxes[0].set_ylim([0, 1])\n\naxes[1].bar(['Simple', 'Multiple'], [rmse_s, rmse_m], color=['steelblue', 'green'])\naxes[1].set_ylabel('RMSE ($)', fontweight='bold')\naxes[1].set_title('RMSE (Lower Better)', fontweight='bold')\n\naxes[2].bar(['Simple', 'Multiple'], [mae_s, mae_m], color=['steelblue', 'green'])\naxes[2].set_ylabel('MAE ($)', fontweight='bold')\naxes[2].set_title('MAE (Lower Better)', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n<a id='conclusions'></a>\n\n## 4.5 Conclusions\n\n### Key Findings\n\n**Simple LR:** Provides interpretable baseline using single best feature\n\n**Multiple LR:** Significantly better performance using all features\n\n### Recommendations\n\n1. Deploy Multiple LR for production use\n2. Model suitable for property valuation\n3. Future: Explore Random Forest, Gradient Boosting\n4. Consider regularization (Ridge, LASSO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Final summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"PROJECT COMPLETE\")\nprint(\"=\"*70)\nprint(f\"Dataset: 2,930 properties\")\nprint(f\"Features: {X.shape[1]}\")\nprint(f\"Best Model: Multiple LR\")\nprint(f\"R\u00b2: {r2_test_m:.4f}\")\nprint(f\"RMSE: ${rmse_m:,.0f}\")\nprint(f\"MAE: ${mae_m:,.0f}\")\nprint(\"=\"*70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Complete\n\nThis analysis successfully developed predictive models for house price estimation.\n\n**All phases completed:**\n- \u2705 Phase 1: Data Acquisition\n- \u2705 Phase 2A: Preprocessing & EDA\n- \u2705 Phase 2B: Feature Engineering\n- \u2705 Phase 3: Modeling & Evaluation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}