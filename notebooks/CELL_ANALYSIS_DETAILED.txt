========================================================================================================================
COMPLETE CELL-BY-CELL ANALYSIS OF PROJECT DELIVERABLE NOTEBOOK
========================================================================================================================

Total Cells: 112
Markdown Cells: 63
Code Cells: 49

========================================================================================================================


========================================================================================================================
CELL #001 | Type: MARKDOWN | ID: c90920ac
========================================================================================================================
CONTENT:
  # Real Estate Price Prediction - Complete Project Deliverable
  

PURPOSE:
  ‚Üí Project header, title, team info, and table of contents


========================================================================================================================
CELL #002 | Type: MARKDOWN | ID: cfc7c90a
========================================================================================================================
CONTENT:
  **Team**: The Outliers  
  **Course**: Advanced Apex Project 1 - BITS Pilani Digital  
  **Trimester**: First Trimester 2025-26  
  **Supervisor**: Bharathi Dasari
  

PURPOSE:
  ‚Üí Project header, title, team info, and table of contents


========================================================================================================================
CELL #003 | Type: MARKDOWN | ID: 8cabc0bb
========================================================================================================================
CONTENT:
  ## Team Members
  
  | Name | BITS ID |
  |------|--------|
  | Anik Das | 2025EM1100026 |
  | Adeetya Wadikar | 2025EM1100384 |
  | Tushar Nishane | 2025EM1100306 |
  

PURPOSE:
  ‚Üí Project header, title, team info, and table of contents


========================================================================================================================
CELL #004 | Type: MARKDOWN | ID: 35a9c874
========================================================================================================================
CONTENT:
  ## Project Overview
  
  **Problem Statement**: Accurate real estate price prediction using machine learning regression models.
  
  **Business Goal**: Develop a predictive model that estimates property sale prices with high accuracy to help buyers, sellers, and investors make informed decisions.
  
  **Dataset**: Ames Housing Dataset from Kaggle  
  - **Records**: 2,930 residential properties  
  - **Features**: 82 attributes (numerical and categorical)  
  - **Target Variable**: SalePrice (house sale price in USD)
  

PURPOSE:
  ‚Üí Project header, title, team info, and table of contents


========================================================================================================================
CELL #005 | Type: MARKDOWN | ID: 377ced68
========================================================================================================================
CONTENT:
  ## Table of Contents
  
  1. [Phase 1: Data Acquisition](#phase1)
  2. [Phase 2A: Data Preprocessing & EDA](#phase2a)
  3. [Phase 2B: Feature Engineering](#phase2b)
  4. [Phase 3: Modeling & Evaluation](#phase3)
  5. [Phase 4: Visualization & Storytelling](#phase4)
  6. [Phase 5: Final Summary & Conclusions](#phase5)
  

PURPOSE:
  ‚Üí Project header, title, team info, and table of contents


========================================================================================================================
CELL #006 | Type: MARKDOWN | ID: 275d980c
========================================================================================================================
CONTENT:
  ---
  <a id='phase1'></a>
  
  # Phase 1: Data Acquisition
  
  **Objective**: Load the Ames Housing dataset, verify its structure, and create a metadata summary for downstream analysis.
  
  **Deliverables**:
  - ‚úÖ Data extraction from Kaggle
  - ‚úÖ Schema verification
  - ‚úÖ Data audit (missing values, duplicates, identifiers)
  - ‚úÖ Metadata summary creation
  - ‚úÖ Data dictionary validation
  

PURPOSE:
  ‚Üí Project header, title, team info, and table of contents


========================================================================================================================
CELL #007 | Type: MARKDOWN | ID: beb405c6
========================================================================================================================
CONTENT:
  ---
  ## 1.1 Import Dependencies

PURPOSE:
  ‚Üí Documentation for importing libraries


========================================================================================================================
CELL #008 | Type: CODE | ID: 2414f3d9
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Import required libraries
   2: import os
   3: import pandas as pd
   4: import numpy as np
   5: import matplotlib.pyplot as plt
   6: import seaborn as sns
   7: # Machine Learning libraries (for Phase 3)
   8: from sklearn.model_selection import train_test_split
   9: from sklearn.linear_model import LinearRegression
  10: from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
  11: # Settings
  12: import warnings
  13: warnings.filterwarnings('ignore')
  14: # Display settings
  15: pd.set_option('display.max_columns', None)
  ... (8 more lines)

WHAT THIS CODE DOES:
  ‚Üí Imports required libraries (pandas, numpy, matplotlib, seaborn, warnings)


========================================================================================================================
CELL #009 | Type: MARKDOWN | ID: 9a05de56
========================================================================================================================
CONTENT:
  ## 1.2 Dataset Import
  
  **Data Source**: Kaggle - Ames Housing Dataset  
  **Citation**: Shashank Necrothapa. (n.d.). Ames Housing Dataset [Data set]. Kaggle.  
  **URL**: https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset
  
  The dataset was downloaded manually from Kaggle and stored in the `data/` directory.

PURPOSE:
  ‚Üí Documentation for importing libraries


========================================================================================================================
CELL #010 | Type: CODE | ID: 2c4e0b46
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Define data path
   2: data_path = "../data/AmesHousing.csv"
   3: # Load dataset
   4: df = pd.read_csv(data_path)
   5: print("‚úÖ Dataset Loaded Successfully")
   6: print(f"Shape: {df.shape}")
   7: print(f"\nDataset contains {df.shape[0]:,} rows and {df.shape[1]} columns")

WHAT THIS CODE DOES:
  ‚Üí Loads the Ames Housing dataset from CSV file
  ‚Üí Displays dataset shape and first few rows


========================================================================================================================
CELL #011 | Type: MARKDOWN | ID: 007a5e46
========================================================================================================================
CONTENT:
  ---
  
  ## Phase 1 Summary
  
  ### ‚úÖ Deliverables Completed:
  
  1. **Dataset Successfully Loaded**
     - Shape: 2,930 rows √ó 82 columns
     - Source: Kaggle - Ames Housing Dataset
  
  2. **Data Audit Completed**
     - No duplicate rows found ‚úÖ
     - 27 features contain missing values
     - Unique identifiers: Order, PID
  
  3. **Data Types Identified**
     - Categorical (object): 43 columns
     - Integer (int64): 28 columns
     - Float (float64): 11 columns
  
  ... (5 more lines)

PURPOSE:
  ‚Üí Phase 1 section header or description


========================================================================================================================
CELL #012 | Type: MARKDOWN | ID: f5a25da3
========================================================================================================================
CONTENT:
  <a id='phase2a'></a>
  
  # Phase 2A: Data Preprocessing & EDA
  
  **Objective**: Clean the dataset, handle missing values, perform exploratory data analysis, and prepare data for feature engineering.
  
  **Deliverables**:
  - ‚úÖ Missing value treatment
  - ‚úÖ Outlier detection and analysis
  - ‚úÖ Univariate and bivariate analysis
  - ‚úÖ Data quality improvements
  - ‚úÖ Cleaned dataset preparation
  
  ---

PURPOSE:
  ‚Üí Phase 2A section header or description


========================================================================================================================
CELL #013 | Type: MARKDOWN | ID: 2ae1fed7
========================================================================================================================
CONTENT:
  ### ===============================================
  ###  01_Data_Extraction.ipynb
  ###  Team: The Outliers
  ###  Course: Advanced Apex Project - BITS Pilani
  ###  Phase: 1 (Data Acquisition)
  ### ===============================================
  

PURPOSE:
  ‚Üí Explanation of outlier detection methodology


========================================================================================================================
CELL #014 | Type: MARKDOWN | ID: c86f084c
========================================================================================================================
CONTENT:
  #### Import Dependencies

PURPOSE:
  ‚Üí Documentation for importing libraries


========================================================================================================================
CELL #015 | Type: MARKDOWN | ID: e1dfcbe6
========================================================================================================================
CONTENT:
  #### Dataset Import

PURPOSE:
  ‚Üí Documentation for importing libraries


========================================================================================================================
CELL #016 | Type: MARKDOWN | ID: 123c0e34
========================================================================================================================
CONTENT:
  #### Verify Data Schema

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #017 | Type: CODE | ID: a18aabf6
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: print("\nColumn Names:\n", df.columns.tolist())
   2: print("\nData Types Summary:\n", df.dtypes.value_counts())

WHAT THIS CODE DOES:
  ‚Üí Verifies data schema
  ‚Üí Lists all column names and data types


========================================================================================================================
CELL #018 | Type: MARKDOWN | ID: bee651c4
========================================================================================================================
CONTENT:
  #### Basic Sanity Checks

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #019 | Type: CODE | ID: b92d3210
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Check how many columns have at least one missing (NaN) value.
   2: # Missing values mean that data is incomplete in some records.
   3: # This helps us identify columns that will need imputation or cleaning in Phase 2.
   4: print("\nAny Missing Values?", df.isnull().any().sum())
   5: # Count duplicate rows in the dataset.
   6: # Duplicate rows are exact copies of other rows and can distort model learning,
   7: # so they should be removed during preprocessing.
   8: print("Duplicate Rows:", df.duplicated().sum())
   9: # Identify columns that have all unique values ‚Äî meaning no two rows share the same value in that column.
  10: # These columns can serve as identifiers (like 'PID' or 'Order'), not as model features.
  11: print("Unique Identifier Columns:", [col for col in df.columns if df[col].is_unique])

WHAT THIS CODE DOES:
  ‚Üí Analyzes missing values across all columns
  ‚Üí Calculates count and percentage of missing data


========================================================================================================================
CELL #020 | Type: MARKDOWN | ID: 7c320a86
========================================================================================================================
CONTENT:
  #### Save Metadata Summary

PURPOSE:
  ‚Üí Summary of phase or section


========================================================================================================================
CELL #021 | Type: CODE | ID: 31ae20d9
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Create a small reference table (DataFrame) that captures key information 
   2: # about every column in the dataset. This will help in Phase 2 
   3: # when we start cleaning and preprocessing the data.
   4: schema_summary = pd.DataFrame({
   5:     # Column name in the dataset
   6:     "Feature": df.columns,
   7:     # Data type of each column (e.g., int64, float64, object)
   8:     # Using .astype(str) to make sure all types are stored as readable text
   9:     "Data Type": df.dtypes.astype(str),
  10:     # Count how many missing (NaN) values each column has
  11:     # This helps us identify which columns need imputation or can be dropped
  12:     "Missing Values": df.isnull().sum(),
  13:     # Count of unique (distinct) values per column
  14:     # This helps detect identifier columns or categorical variables
  15:     "Unique Values": [df[col].nunique() for col in df.columns]
  ... (6 more lines)

WHAT THIS CODE DOES:
  ‚Üí Verifies data schema
  ‚Üí Lists all column names and data types


========================================================================================================================
CELL #022 | Type: MARKDOWN | ID: 381b3437
========================================================================================================================
CONTENT:
  #### Cross-Check with Data Dictionary

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #023 | Type: CODE | ID: d585f652
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Load the data dictionary Excel file
   2: # (Requires openpyxl to read .xlsx files)
   3: data_dict = pd.read_excel("../docs/data_dictionary.xlsx")
   4: # Compare column names between dataset and dictionary
   5: missing_cols = set(df.columns) - set(data_dict["Feature"])
   6: # Print out any columns that exist in dataset but not in dictionary
   7: print("Columns missing in data dictionary:", missing_cols)

WHAT THIS CODE DOES:
  ‚Üí Loads data dictionary from Excel file
  ‚Üí Cross-references with actual dataset schema


========================================================================================================================
CELL #024 | Type: MARKDOWN | ID: 86750a40
========================================================================================================================
CONTENT:
  <a id='phase2b'></a>
  
  # Phase 2B: Feature Engineering
  
  **Objective**: Create new features, transform existing features, and prepare the final dataset for modeling.
  
  **Deliverables**:
  - ‚úÖ Feature creation and transformation
  - ‚úÖ Encoding categorical variables
  - ‚úÖ Feature scaling and normalization
  - ‚úÖ Final dataset preparation
  
  ---

PURPOSE:
  ‚Üí Phase 2B section header or description


========================================================================================================================
CELL #025 | Type: MARKDOWN | ID: 5781c385
========================================================================================================================
CONTENT:
  ### ===============================================
  ###  02_Preprocessing_EDA.ipynb
  ###  Team: The Outliers
  ###  Course: Advanced Apex Project - BITS Pilani
  ###  Phase: 2 (Preprocessing & Exploratory Data Analysis)
  ### ===============================================
  

PURPOSE:
  ‚Üí Explanation of outlier detection methodology


========================================================================================================================
CELL #026 | Type: MARKDOWN | ID: 6a87e788
========================================================================================================================
CONTENT:
  #### Notebook Header + Imports

PURPOSE:
  ‚Üí Documentation for importing libraries


========================================================================================================================
CELL #027 | Type: CODE | ID: cd4ea1ed
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Display basic info
   2: print("‚úÖ Dataset loaded successfully!")
   3: print("Shape:", df.shape)
   4: print("\nColumns:", len(df.columns))
   5: df.head(3)

WHAT THIS CODE DOES:
  ‚Üí Displays basic dataset information
  ‚Üí Shows shape, column count, data types


========================================================================================================================
CELL #028 | Type: MARKDOWN | ID: d356370a
========================================================================================================================
CONTENT:
  #### Data Types, Summary Statistics & Missing Value Overview
  
  #### ===============================================
  #### üîç Data Overview and Structure Analysis
  #### ===============================================

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #029 | Type: CODE | ID: 5b0355b2
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 1Ô∏è‚É£ Basic info about each column (data types, non-null counts)
   2: print("üîπ Basic Dataset Info:\n")
   3: df.info()

WHAT THIS CODE DOES:
  ‚Üí (General code execution - see preview above for details)


========================================================================================================================
CELL #030 | Type: CODE | ID: 6a5050f0
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 2Ô∏è‚É£ Quick statistical summary of numeric columns
   2: print("\nüîπ Summary Statistics (Numerical Features):\n")
   3: display(df.describe().T)

WHAT THIS CODE DOES:
  ‚Üí Generates summary statistics for numerical features
  ‚Üí Shows mean, std, min, max, quartiles


========================================================================================================================
CELL #031 | Type: CODE | ID: f9ba74cf
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 3Ô∏è‚É£ Summary of data types (to know how many are numeric vs categorical)
   2: print("\nüîπ Data Type Counts:\n")
   3: print(df.dtypes.value_counts())

WHAT THIS CODE DOES:
  ‚Üí Verifies data schema
  ‚Üí Lists all column names and data types


========================================================================================================================
CELL #032 | Type: CODE | ID: 286ae079
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 4Ô∏è‚É£ Check missing values in each column (sorted descending)
   2: print("\nüîπ Missing Values Overview:\n")
   3: missing_summary = df.isnull().sum().sort_values(ascending=False)
   4: display(missing_summary.head(15))  # show top 15 columns with most missing values

WHAT THIS CODE DOES:
  ‚Üí Analyzes missing values across all columns
  ‚Üí Calculates count and percentage of missing data


========================================================================================================================
CELL #033 | Type: MARKDOWN | ID: 7b4d4b2b
========================================================================================================================
CONTENT:
  - Columns like Pool QC, Alley, Misc Feature, Fence have >80% missing values ‚Äî candidates for dropping.
  - Others (Garage, Basement, Fireplace) will be imputed contextually.
  
  We‚Äôll now visualize the missing value pattern to confirm this visually ‚Äî it helps us see whether missingness is random or structured.

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #034 | Type: MARKDOWN | ID: 84948b9c
========================================================================================================================
CONTENT:
  #### Missing Value Visualization
  
  #### ===============================================
  #### üìä Visualizing Missing Data Patterns
  #### ===============================================

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #035 | Type: CODE | ID: ab09f7ad
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: import missingno as msno
   2: # Visualize missing data as a matrix
   3: plt.figure(figsize=(12,6))
   4: msno.matrix(df)
   5: plt.title("Missing Data Pattern (Matrix View)")
   6: plt.show()

WHAT THIS CODE DOES:
  ‚Üí Visualizes missing data patterns
  ‚Üí Creates bar chart or matrix view of missing values


========================================================================================================================
CELL #036 | Type: MARKDOWN | ID: 6957dd29
========================================================================================================================
CONTENT:
  #### Matrix View 
  
  - Each column = a dataset feature.
  
  - Each row = a house entry.
  
  - White gaps = missing values.
  
  - we can see long white vertical lines in some columns ‚Üí that‚Äôs consistent missing data.
  
  #### üß† Observation:
  
  - Pool QC, Misc Feature, Alley, and Fence have large continuous white sections ‚Äî meaning they are missing for most rows.
  
  - Basement and garage columns have partial missingness ‚Äî not random, likely due to houses without basements or garages.

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #037 | Type: CODE | ID: 36dbdee0
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Visualize missing data as a bar chart
   2: plt.figure(figsize=(10,5))
   3: msno.bar(df)
   4: plt.title("Missing Values per Feature (Bar View)")
   5: plt.show()

WHAT THIS CODE DOES:
  ‚Üí Visualizes missing data patterns
  ‚Üí Creates bar chart or matrix view of missing values


========================================================================================================================
CELL #038 | Type: MARKDOWN | ID: c0afeda9
========================================================================================================================
CONTENT:
  ### Bar View
  
  - The longer the white bar, the higher the percentage of missing values.
  
  - Confirms the same pattern numerically:
  
      - Pool QC, Misc Feature, Alley, Fence ‚Üí >80% missing
  
      - Mas Vnr Type, Fireplace Qu ‚Üí moderate missingness
  
      - Garage and Basement groups ‚Üí small but meaningful gaps.

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #039 | Type: MARKDOWN | ID: 9867248b
========================================================================================================================
CONTENT:
  #### Handle Missing Values (Drop + Impute)
  #### ===============================================
  #### üßπ Handling Missing Values
  #### ===============================================

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #040 | Type: CODE | ID: 91e099d7
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 1Ô∏è‚É£ Drop columns with too many missing values (>80%)
   2: cols_to_drop = ["Pool QC", "Misc Feature", "Alley", "Fence"]
   3: df.drop(columns=cols_to_drop, inplace=True)
   4: print(f"Dropped columns with excessive missing values: {cols_to_drop}")

WHAT THIS CODE DOES:
  ‚Üí Drops columns with excessive missing values (>80%)
  ‚Üí Removes: Pool QC, Misc Feature, Alley, Fence


========================================================================================================================
CELL #041 | Type: CODE | ID: 18655188
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 2Ô∏è‚É£ Impute categorical columns based on meaning
   2: # Replace missing values in certain categorical columns with 'None' or domain-specific category
   3: categorical_impute_none = [
   4:     "Mas Vnr Type", "Fireplace Qu", "Garage Type", "Garage Finish",
   5:     "Garage Qual", "Garage Cond", "Bsmt Qual", "Bsmt Cond",
   6:     "Bsmt Exposure", "BsmtFin Type 1", "BsmtFin Type 2"
   7: ]
   8: df[categorical_impute_none] = df[categorical_impute_none].fillna("None")

WHAT THIS CODE DOES:
  ‚Üí Imputes categorical features with 'None' (indicating absence)
  ‚Üí Handles: Garage, Basement, Fireplace features


========================================================================================================================
CELL #042 | Type: CODE | ID: d036d48a
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 3Ô∏è‚É£ Impute numerical columns (fill missing with 0 for areas / counts)
   2: numeric_impute_zero = [
   3:     "Mas Vnr Area", "BsmtFin SF 1", "BsmtFin SF 2", "Bsmt Unf SF",
   4:     "Total Bsmt SF", "Bsmt Full Bath", "Bsmt Half Bath",
   5:     "Garage Yr Blt", "Garage Cars", "Garage Area"
   6: ]
   7: df[numeric_impute_zero] = df[numeric_impute_zero].fillna(0)

WHAT THIS CODE DOES:
  ‚Üí Imputes numerical features with 0 (areas/counts where absence = 0)
  ‚Üí Handles: Masonry, Basement SF, Garage area/cars, etc.


========================================================================================================================
CELL #043 | Type: CODE | ID: 67be4677
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 4Ô∏è‚É£ Impute Lot Frontage with median per neighborhood (since it depends on location)
   2: df["Lot Frontage"] = df.groupby("Neighborhood")["Lot Frontage"].transform(lambda x: x.fillna(x.median()))

WHAT THIS CODE DOES:
  ‚Üí Imputes Lot Frontage using neighborhood-grouped median
  ‚Üí More accurate than global median


========================================================================================================================
CELL #044 | Type: CODE | ID: c14413e3
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # 5Ô∏è‚É£ Electrical has one missing value; fill with mode
   2: df["Electrical"] = df["Electrical"].fillna(df["Electrical"].mode()[0])

WHAT THIS CODE DOES:
  ‚Üí Imputes single missing Electrical value with mode


========================================================================================================================
CELL #045 | Type: CODE | ID: bda588e6
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ‚úÖ Verify no missing values remain
   2: print("\nRemaining Missing Values:\n", df.isnull().sum().sum())

WHAT THIS CODE DOES:
  ‚Üí Analyzes missing values across all columns
  ‚Üí Calculates count and percentage of missing data


========================================================================================================================
CELL #046 | Type: MARKDOWN | ID: fbc03973
========================================================================================================================
CONTENT:
  #### Univariate Analysis ‚Äì Numerical Features
  #### ===============================================
  #### üìä Univariate Analysis - Numerical Features
  #### ===============================================

PURPOSE:
  ‚Üí Description of univariate analysis section


========================================================================================================================
CELL #047 | Type: CODE | ID: d4803a9c
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Select only numeric columns for visualization
   2: numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

WHAT THIS CODE DOES:
  ‚Üí Verifies data schema
  ‚Üí Lists all column names and data types


========================================================================================================================
CELL #048 | Type: CODE | ID: 9e248958
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Exclude identifier columns (Order, PID)
   2: numeric_cols = [col for col in numeric_cols if col not in ["Order", "PID"]]

WHAT THIS CODE DOES:
  ‚Üí (General code execution - see preview above for details)


========================================================================================================================
CELL #049 | Type: CODE | ID: 4499d64f
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Plot histograms for numerical features
   2: df[numeric_cols].hist(figsize=(20, 20), bins=30, color="#4B8BBE", edgecolor="black")
   3: plt.suptitle("Distribution of Numerical Features", fontsize=16)
   4: plt.show()

WHAT THIS CODE DOES:
  ‚Üí (General code execution - see preview above for details)


========================================================================================================================
CELL #050 | Type: MARKDOWN | ID: 324193ce
========================================================================================================================
CONTENT:
  | Feature                            | Distribution Shape   | Interpretation                                                                |
  | ---------------------------------- | -------------------- | ----------------------------------------------------------------------------- |
  | **Lot Area**                       | Right-skewed         | Most houses have small lots; a few large properties inflate range (outliers). |
  | **SalePrice**                      | Right-skewed         | Common in housing prices ‚Äî a few luxury homes pull the mean upward.           |
  | **Gr Liv Area**, **Total Bsmt SF** | Right-skewed         | Larger homes are fewer in number.                                             |
  | **Overall Qual**                   | Slightly left-skewed | More high-quality homes (6‚Äì8 range) than poor-quality ones.                   |
  | **Year Built**, **Year Remod/Add** | Multi-peaked         | Reflects waves of development (old houses + modern builds).                   |
  | **Garage Cars**, **Full Bath**     | Discrete peaks       | Most homes have 1‚Äì2 garages, 1‚Äì2 full baths.                                  |
  | **Mo Sold**                        | Uniform-like         | Sales occur year-round; a few seasonal peaks (e.g., summer).                  |
  

PURPOSE:
  ‚Üí Interpretation and analysis of results


========================================================================================================================
CELL #051 | Type: MARKDOWN | ID: 13985d73
========================================================================================================================
CONTENT:
  #### Univariate Analysis (Categorical Features)
  #### ===============================================
  #### üìä Univariate Analysis - Categorical Features
  #### ===============================================
  

PURPOSE:
  ‚Üí Description of univariate analysis section


========================================================================================================================
CELL #052 | Type: CODE | ID: eaec9302
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Select only categorical columns
   2: categorical_cols = df.select_dtypes(include=["object"]).columns.tolist()
   3: # Plot countplots for each categorical column
   4: fig, axes = plt.subplots(nrows=len(categorical_cols)//3 + 1, ncols=3, figsize=(18, 60))
   5: axes = axes.flatten()
   6: for i, col in enumerate(categorical_cols):
   7:     sns.countplot(data=df, x=col, ax=axes[i], palette="crest")
   8:     axes[i].set_title(f"Distribution of {col}", fontsize=10)
   9:     axes[i].tick_params(axis="x", rotation=45)
  10:     axes[i].set_xlabel("")
  11:     axes[i].set_ylabel("Count")
  12: plt.tight_layout()
  13: plt.show()

WHAT THIS CODE DOES:
  ‚Üí Verifies data schema
  ‚Üí Lists all column names and data types


========================================================================================================================
CELL #053 | Type: MARKDOWN | ID: 908152e7
========================================================================================================================
CONTENT:
  in the dataset using count plots to understand their frequency distributions and balance across categories. This helps identify dominant categories, rare classes, and low-variance features that provide little or no predictive value.
  
  * Several features show **diverse and meaningful variation**, indicating they are likely to contribute to predicting `SalePrice`.
    Examples:
  
    * `MS Zoning` ‚Üí Multiple zoning categories (RL, RM, FV, etc.)
    * `Neighborhood` ‚Üí Distinct distribution across 20+ neighborhoods
    * `Bldg Type`, `House Style`, `Exterior 1st/2nd`, `Garage Type`, `Kitchen Qual`, `Functional`, etc.
  
  * A few features are **highly imbalanced**, dominated by one category, and thus carry very little information for modeling:
  
    | Feature     | Dominant Category | Interpretation                       |
    | ----------- | ----------------- | ------------------------------------ |
    | Street      | Pave (~100%)      | Almost all houses are on paved roads |
    | Utilities   | AllPub (~99%)     | No variation                         |
    | Land Slope  | Gtl (~98%)        | Minimal variation                    |
    | Condition 2 | Norm (~99%)       | Very low variance                    |
    | Roof Matl   | CompShg (~98%)    | Homogeneous feature                  |
    | Heating     | GasA (~99%)       | Single-category dominance            |
  
  ... (7 more lines)

PURPOSE:
  ‚Üí Interpretation and analysis of results


========================================================================================================================
CELL #054 | Type: MARKDOWN | ID: e1fd8fc8
========================================================================================================================
CONTENT:
  #### Drop Uninformative Categorical Features
  #### ===============================================
  #### üßπ Drop Low-Variance Categorical Features
  #### ===============================================

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #055 | Type: CODE | ID: 90bace10
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Features dominated by one category (low information)
   2: low_variance_cols = ["Street", "Utilities", "Condition 2", "Roof Matl", "Heating", "Land Slope"]
   3: # Drop them
   4: df.drop(columns=low_variance_cols, inplace=True)
   5: print(f"Dropped low-variance columns: {low_variance_cols}")
   6: # Verify column count after dropping
   7: print("Remaining columns:", len(df.columns))

WHAT THIS CODE DOES:
  ‚Üí Drops low-variance categorical features (dominated by one category)
  ‚Üí Removes: Street, Utilities, Condition 2, Roof Matl, Heating, Land Slope


========================================================================================================================
CELL #056 | Type: MARKDOWN | ID: 0c12c598
========================================================================================================================
CONTENT:
  | Step                     | Action                                                                         | Result                                                         |
  | ------------------------ | ------------------------------------------------------------------------------ | -------------------------------------------------------------- |
  | üßπ **Dropped Columns**   | `['Street', 'Utilities', 'Condition 2', 'Roof Matl', 'Heating', 'Land Slope']` | These features had ~99% of one category (no predictive power). |
  | üìâ **Remaining Columns** | 72                                                                             | A clean, balanced dataset ‚Äî ready for deeper analysis.         |
  

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #057 | Type: MARKDOWN | ID: 90209789
========================================================================================================================
CONTENT:
  #### Bivariate Analysis (Correlation & Price Relationships)
  #### ===============================================
  #### üîó Bivariate Analysis - Correlation Heatmap
  #### ===============================================

PURPOSE:
  ‚Üí Description of bivariate analysis and correlations


========================================================================================================================
CELL #058 | Type: CODE | ID: bd797db0
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Compute correlation matrix for numeric features
   2: corr_matrix = df.corr(numeric_only=True)
   3: # Display top 10 most correlated features with SalePrice
   4: print("üîπ Top 10 Features Most Correlated with SalePrice:\n")
   5: print(corr_matrix["SalePrice"].sort_values(ascending=False).head(11))
   6: # Plot heatmap of correlations among top correlated features
   7: plt.figure(figsize=(12, 8))
   8: top_corr_features = corr_matrix["SalePrice"].sort_values(ascending=False).head(11).index
   9: sns.heatmap(df[top_corr_features].corr(), annot=True, cmap="coolwarm", fmt=".2f")
  10: plt.title("Correlation Heatmap of Top Features with SalePrice", fontsize=14)
  11: plt.show()

WHAT THIS CODE DOES:
  ‚Üí Calculates correlation matrix for numeric features
  ‚Üí Creates heatmap visualization
  ‚Üí Identifies features most correlated with SalePrice


========================================================================================================================
CELL #059 | Type: MARKDOWN | ID: 01b38d3c
========================================================================================================================
CONTENT:
  we examined the pairwise correlations between numerical variables and the target variable, SalePrice.
  The goal is to identify which features have the strongest linear relationships with housing prices.
  
  üßÆ Top 10 Features Most Correlated with SalePrice
  
  | Rank | Feature            | Correlation | Interpretation                                                                         |
  | :--: | :----------------- | :---------- | :------------------------------------------------------------------------------------- |
  |   1  | **Overall Qual**   | 0.80        | Strongest predictor ‚Äî higher overall material/finish quality strongly increases price. |
  |   2  | **Gr Liv Area**    | 0.71        | Larger above-ground living areas lead to higher sale prices.                           |
  |   3  | **Garage Cars**    | 0.65        | Number of cars the garage can hold shows strong influence.                             |
  |   4  | **Garage Area**    | 0.64        | Correlated with `Garage Cars`, both indicate garage capacity.                          |
  |   5  | **Total Bsmt SF**  | 0.63        | Larger basements contribute positively to house value.                                 |
  |   6  | **1st Flr SF**     | 0.62        | Bigger ground floors correlate with higher prices.                                     |
  |   7  | **Year Built**     | 0.56        | Newer homes tend to be more valuable.                                                  |
  |   8  | **Full Bath**      | 0.55        | More bathrooms add to property worth.                                                  |
  |   9  | **Year Remod/Add** | 0.53        | Renovated homes fetch higher prices.                                                   |
  |  10  | **Mas Vnr Area**   | 0.50        | Masonry veneer area has moderate impact on price.                                      |
  
  
  Heatmap Interpretation
  ... (17 more lines)

PURPOSE:
  ‚Üí Description of bivariate analysis and correlations


========================================================================================================================
CELL #060 | Type: MARKDOWN | ID: e9480fcf
========================================================================================================================
CONTENT:
  #### Bivariate Visualizations (Numeric vs. SalePrice)
  #### ===============================================
  #### üìà Bivariate Visualizations - Continuous Features vs. SalePrice
  #### ===============================================

PURPOSE:
  ‚Üí Description of bivariate analysis and correlations


========================================================================================================================
CELL #061 | Type: CODE | ID: 23353d02
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # List of top continuous features to visualize
   2: top_features = ["Gr Liv Area", "Total Bsmt SF", "Garage Area", "1st Flr SF", "Mas Vnr Area"]
   3: plt.figure(figsize=(15, 10))
   4: for i, feature in enumerate(top_features):
   5:     plt.subplot(2, 3, i + 1)
   6:     sns.scatterplot(data=df, x=feature, y="SalePrice", alpha=0.6, color="#1f77b4")
   7:     plt.title(f"{feature} vs SalePrice")
   8:     plt.xlabel(feature)
   9:     plt.ylabel("SalePrice")
  10: plt.tight_layout()
  11: plt.show()

WHAT THIS CODE DOES:
  ‚Üí Creates scatter plots of top features vs SalePrice
  ‚Üí Visualizes bivariate relationships


========================================================================================================================
CELL #062 | Type: MARKDOWN | ID: 9c8c47bf
========================================================================================================================
CONTENT:
  
  ### üìä **1. Gr Liv Area vs SalePrice**
  
  * Clear **positive linear relationship** ‚Äî as living area increases, so does the sale price.
  * A few extreme outliers (very large homes with moderate prices) are visible ‚Äî we‚Äôll address these in the outlier handling step.
  
  ### üß± **2. Total Bsmt SF vs SalePrice**
  
  * Strong upward trend: homes with larger basements tend to have higher prices.
  * Some zero-basement homes visible at the bottom, forming a cluster near `0`.
  
  ### üöó **3. Garage Area vs SalePrice**
  
  * Positive linear pattern ‚Äî larger garages often indicate higher-quality or newer homes.
  * Slight variance for mid-sized garages, but trend remains consistent.
  
  ### üè† **4. 1st Flr SF vs SalePrice**
  
  * Again, a steady increase in price with larger ground floor area.
  * Some spread at higher square footage levels ‚Äî likely due to other factors (like location or overall quality).
  ... (15 more lines)

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #063 | Type: MARKDOWN | ID: 1d3fd30c
========================================================================================================================
CONTENT:
  ### üìà **Bivariate Analysis ‚Äì Continuous Features vs. SalePrice**
  
  We plotted `SalePrice` against the most correlated continuous variables to visually inspect their relationships.
  
  #### üß† **Observations**
  
  * All selected features show **positive linear trends** with `SalePrice`.
  * The **larger the size or area**, the **higher the property‚Äôs sale value**.
  * Key drivers include:
  
    * **Gr Liv Area** (above-ground living area)
    * **Total Bsmt SF** (basement size)
    * **Garage Area / 1st Flr SF** (space-based quality indicators)
  * A few **outliers** exist (e.g., extremely large properties with moderate prices), which will be reviewed during the outlier analysis.
  
  #### ‚úÖ **Conclusion**
  
  * The scatterplots confirm strong positive relationships between house size-related features and sale price.
  * These features will play a **critical role in model building** and **feature engineering**.
  * Next, we will explore **categorical/ordinal features** using **boxplots** to compare average `SalePrice` across quality and condition categories.
  ... (1 more lines)

PURPOSE:
  ‚Üí Description of bivariate analysis and correlations


========================================================================================================================
CELL #064 | Type: MARKDOWN | ID: e4c0c9c4
========================================================================================================================
CONTENT:
  ### Boxplots for Categorical/Ordinal Features
  ##### ===============================================
  #### üì¶ Bivariate Analysis - Categorical/Ordinal Features vs. SalePrice
  ##### ===============================================

PURPOSE:
  ‚Üí Description of bivariate analysis and correlations


========================================================================================================================
CELL #065 | Type: CODE | ID: cfb0c259
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Select important ordinal/categorical variables to visualize
   2: categorical_features = [
   3:     "Overall Qual", "Overall Cond", "Kitchen Qual",
   4:     "Exter Qual", "Bsmt Qual", "Garage Finish", "Functional", "Sale Condition"
   5: ]
   6: plt.figure(figsize=(16, 20))
   7: for i, feature in enumerate(categorical_features):
   8:     plt.subplot(4, 2, i + 1)
   9:     sns.boxplot(data=df, x=feature, y="SalePrice", palette="crest")
  10:     plt.title(f"SalePrice vs {feature}", fontsize=12)
  11:     plt.xlabel(feature)
  12:     plt.ylabel("SalePrice")
  13:     plt.xticks(rotation=45)
  14: plt.tight_layout()
  15: plt.show()

WHAT THIS CODE DOES:
  ‚Üí Creates scatter plots of top features vs SalePrice
  ‚Üí Visualizes bivariate relationships


========================================================================================================================
CELL #066 | Type: MARKDOWN | ID: 353e2c72
========================================================================================================================
CONTENT:
  üß© **Interpretation: Categorical / Ordinal Features vs. SalePrice**
  
  our boxplots clearly demonstrate the **impact of qualitative ratings** (like quality, condition, and finish) on house prices.
  Here‚Äôs the structured interpretation you can include üëá
  
  ---
  
  ### üì¶ **Bivariate Analysis ‚Äì Categorical / Ordinal Features vs. SalePrice**
  
  This section explores how **qualitative features** such as material quality, condition, and functionality influence the house sale price (`SalePrice`).
  Each boxplot shows the **median, quartile range, and outliers** for different categories of the feature.
  
  #### üß† **Key Observations**
  
  | Feature            | Relationship with SalePrice | Insights                                                                                                                                |
  | ------------------ | --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
  | **Overall Qual**   | üî∫ Strong Positive          | Houses with higher overall quality ratings (8‚Äì10) command significantly higher prices. This is the most powerful categorical predictor. |
  | **Overall Cond**   | ‚öñÔ∏è Moderate                 | Good condition (ratings 5‚Äì8) shows higher price medians, but the trend is less linear than quality.                                     |
  | **Kitchen Qual**   | üî∫ Strong                   | Kitchen quality (`Ex` > `Gd` > `TA` > `Fa`) shows a clear price gradient. High-end kitchens add substantial value.                      |
  | **Exter Qual**     | üî∫ Positive                 | Better exterior materials and finishes increase property value.                                                                         |
  ... (18 more lines)

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #067 | Type: MARKDOWN | ID: 0987131a
========================================================================================================================
CONTENT:
  ### Outlier Detection using IQR (Interquartile Range) Method
  #### ===============================================
  #### üö® Outlier Detection - IQR Method
  #### ===============================================
  

PURPOSE:
  ‚Üí Explanation of outlier detection methodology


========================================================================================================================
CELL #068 | Type: CODE | ID: f0a62b3e
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Select key numeric columns for outlier analysis
   2: key_numeric_features = ["SalePrice", "Gr Liv Area", "Lot Area", "Total Bsmt SF", "Garage Area"]
   3: # Function to detect outliers using IQR method
   4: def detect_outliers(df, feature):
   5:     Q1 = df[feature].quantile(0.25)
   6:     Q3 = df[feature].quantile(0.75)
   7:     IQR = Q3 - Q1
   8:     lower_bound = Q1 - 1.5 * IQR
   9:     upper_bound = Q3 + 1.5 * IQR
  10:     outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]
  11:     return outliers
  12: # Identify and count outliers per feature
  13: for feature in key_numeric_features:
  14:     outliers = detect_outliers(df, feature)
  15:     print(f"{feature}: {len(outliers)} outliers detected")
  ... (8 more lines)

WHAT THIS CODE DOES:
  ‚Üí Creates scatter plots of top features vs SalePrice
  ‚Üí Visualizes bivariate relationships


========================================================================================================================
CELL #069 | Type: MARKDOWN | ID: 4687756b
========================================================================================================================
CONTENT:
  
  
  ## üß© **Interpretation: Outlier Detection Results**
  
  | Feature           | Outliers Detected | Interpretation                                                                           |
  | ----------------- | ----------------- | ---------------------------------------------------------------------------------------- |
  | **SalePrice**     | 137               | A few luxury or abnormally priced homes; likely genuine high-end properties, not errors. |
  | **Gr Liv Area**   | 75                | Large luxury houses (above 4000 sq ft) cause long right tail ‚Üí common in Ames dataset.   |
  | **Lot Area**      | 127               | A few large properties with huge plots; these can heavily skew model results.            |
  | **Total Bsmt SF** | 124               | Unusually large basements (> 3000 sq ft) ‚Äî may need capping.                             |
  | **Garage Area**   | 42                | Oversized garages or multiple garages; relatively mild outliers.                         |
  
  üß† **What this means:**
  
  * These are **not data errors**, but **natural extremes** in the real-estate market.
  * However, they can distort regression models, so we‚Äôll **treat them carefully** ‚Äî usually by *capping* extreme values using IQR limits instead of removing them outright.
  
  ---
  
  ## üìà **Visual Review**
  ... (24 more lines)

PURPOSE:
  ‚Üí Explanation of outlier detection methodology


========================================================================================================================
CELL #070 | Type: MARKDOWN | ID: 8737d6f9
========================================================================================================================
CONTENT:
  <a id='setup'></a>
  
  ## 1. Setup & Data Loading
  
  We'll load the cleaned dataset from Phase 2B and prepare it for feature engineering.
  

PURPOSE:
  ‚Üí Phase 2B section header or description


========================================================================================================================
CELL #071 | Type: MARKDOWN | ID: 6431974a
========================================================================================================================
CONTENT:
  ---
  <a id='feature-creation'></a>
  
  ## 2. Feature Creation & Transformation
  
  We will create new features that are relevant to predicting house prices. These include:
  1. **Total Square Footage** - Combining all living areas
  2. **House Age** - Age of the house at time of sale
  3. **Remodel Age** - Years since last remodel
  4. **Total Bathrooms** - Sum of all bathroom counts
  5. **Total Porch Area** - Combined porch square footage
  6. **Has Pool/Garage/Basement** - Binary indicators
  7. **Quality-Area Interactions** - Interaction between quality and size
  8. **Log Transformations** - For skewed numerical features
  
  **Justification**: These features capture important aspects of house value such as total living space, property age, amenities, and quality-size relationships that are known to influence real estate prices.
  
  ---
  

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #072 | Type: CODE | ID: 9e9fe6de
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ============================================
   2: # 2.1 Create Aggregate Features
   3: # ============================================
   4: print("Creating new features...")
   5: # 1. Total Square Footage (all living areas combined)
   6: df['Total_SF'] = df['Total Bsmt SF'] + df['1st Flr SF'] + df['2nd Flr SF']
   7: # 2. Total Bathrooms (full + half bathrooms)
   8: df['Total_Bathrooms'] = df['Full Bath'] + df['Bsmt Full Bath'] + 0.5 * (df['Half Bath'] + df['Bsmt Half Bath'])
   9: # 3. Total Porch Area (all porch types)
  10: df['Total_Porch_SF'] = df['Open Porch SF'] + df['Enclosed Porch'] + df['3Ssn Porch'] + df['Screen Porch']
  11: # 4. House Age (at time of sale)
  12: df['House_Age'] = df['Yr Sold'] - df['Year Built']
  13: # 5. Years Since Remodel
  14: df['Years_Since_Remod'] = df['Yr Sold'] - df['Year Remod/Add']
  15: # 6. Binary indicators for amenities
  ... (17 more lines)

WHAT THIS CODE DOES:
  ‚Üí Creates engineered features
  ‚Üí Combines related features (bathrooms, porch areas, etc.)
  ‚Üí Creates derived features (house age, years since remodel)


========================================================================================================================
CELL #073 | Type: CODE | ID: 008f6661
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ============================================
   2: # 2.2 Apply Log Transformations to Skewed Features
   3: # ============================================
   4: print("Applying log transformations to skewed numerical features...")
   5: # Identify skewed features (those with high positive skewness)
   6: # Log transformation helps normalize right-skewed distributions
   7: skewed_features = ['Lot Area', 'Gr Liv Area', 'Total_SF', 'SalePrice']
   8: for feature in skewed_features:
   9:     if feature in df.columns:
  10:         # Add 1 to avoid log(0) issues
  11:         df[f'{feature}_Log'] = np.log1p(df[feature])
  12:         print(f"  - Created {feature}_Log")
  13: print(f"\n‚úÖ Applied log transformations to {len(skewed_features)} features")
  14: # Visualize the effect of log transformation on SalePrice
  15: fig, axes = plt.subplots(1, 2, figsize=(12, 4))
  ... (13 more lines)

WHAT THIS CODE DOES:
  ‚Üí Creates scatter plots of top features vs SalePrice
  ‚Üí Visualizes bivariate relationships


========================================================================================================================
CELL #074 | Type: CODE | ID: baa6aa4e
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ============================================
   2: # 2.3 Encode Categorical Variables
   3: # ============================================
   4: print("Encoding categorical variables...")
   5: # Separate categorical columns
   6: categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
   7: print(f"Found {len(categorical_cols)} categorical columns")
   8: # Remove identifier columns if present
   9: categorical_cols = [col for col in categorical_cols if col not in ['Order', 'PID']]
  10: # Create a copy for encoding
  11: df_encoded = df.copy()
  12: # Label Encoding for ordinal categorical variables
  13: # These have a natural order (e.g., quality ratings)
  14: ordinal_mappings = {
  15:     'Exter Qual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  ... (33 more lines)

WHAT THIS CODE DOES:
  ‚Üí Verifies data schema
  ‚Üí Lists all column names and data types


========================================================================================================================
CELL #075 | Type: MARKDOWN | ID: 95ee1b3c
========================================================================================================================
CONTENT:
  ---
  <a id='feature-selection'></a>
  
  ## 3. Feature Selection & Dimensionality Reduction
  
  We will apply multiple feature selection techniques to identify the most important features for predicting house prices:
  
  1. **Correlation-based filtering** - Remove highly correlated features (multicollinearity)
  2. **Statistical tests** - SelectKBest with f_regression
  3. **Feature importance** - Using Random Forest
  4. **Recursive Feature Elimination (RFE)** - Wrapper method
  
  **Justification**: Feature selection helps reduce overfitting, improve model performance, decrease training time, and enhance model interpretability by focusing on the most relevant predictors.
  
  ---
  

PURPOSE:
  ‚Üí Description of bivariate analysis and correlations


========================================================================================================================
CELL #076 | Type: CODE | ID: c8a96e37
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ============================================
   2: # 3.1 Correlation-Based Feature Selection
   3: # ============================================
   4: print("Analyzing feature correlations...")
   5: # Prepare features (exclude identifiers and target)
   6: exclude_cols = ['Order', 'PID', 'SalePrice', 'Price_per_SF']
   7: feature_cols = [col for col in df_encoded.columns if col not in exclude_cols]
   8: # Calculate correlation matrix
   9: correlation_matrix = df_encoded[feature_cols].corr().abs()
  10: # Find highly correlated feature pairs (correlation > 0.85)
  11: upper_triangle = correlation_matrix.where(
  12:     np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
  13: )
  14: # Identify features to drop (keep one from each highly correlated pair)
  15: to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.85)]
  ... (24 more lines)

WHAT THIS CODE DOES:
  ‚Üí Creates scatter plots of top features vs SalePrice
  ‚Üí Visualizes bivariate relationships


========================================================================================================================
CELL #077 | Type: CODE | ID: 1967c0bb
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ============================================
   2: # 3.2 Feature Importance using Random Forest
   3: # ============================================
   4: print("Calculating feature importance using Random Forest...")
   5: # Prepare data for modeling (exclude target and its transformations)
   6: exclude_for_modeling = ['Order', 'PID', 'SalePrice', 'Price_per_SF', 'SalePrice_Log']
   7: feature_cols_for_model = [col for col in df_reduced.columns if col not in exclude_for_modeling]
   8: X = df_reduced[feature_cols_for_model]
   9: y = df_reduced['SalePrice']
  10: print(f"Training Random Forest with {len(feature_cols_for_model)} features...")
  11: # Train a Random Forest model
  12: rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
  13: rf_model.fit(X, y)
  14: # Get feature importances
  15: feature_importance = pd.DataFrame({
  ... (17 more lines)

WHAT THIS CODE DOES:
  ‚Üí Creates scatter plots of top features vs SalePrice
  ‚Üí Visualizes bivariate relationships


========================================================================================================================
CELL #078 | Type: MARKDOWN | ID: 291d33aa
========================================================================================================================
CONTENT:
  ---
  <a id='feature-evaluation'></a>
  
  ## 4. Feature Evaluation & Quick Checks
  
  We will validate that our engineered features are meaningful and useful for prediction by:
  
  1. **Correlation analysis** - Check correlation of new features with target
  2. **Quick baseline model** - Test predictive power with a simple Linear Regression
  3. **Feature comparison** - Compare model performance with/without engineered features
  
  **Justification**: These checks demonstrate that our feature engineering efforts have improved the dataset's predictive capability and that the new features add value beyond the original features.
  
  ---
  

PURPOSE:
  ‚Üí Description of bivariate analysis and correlations


========================================================================================================================
CELL #079 | Type: CODE | ID: 6e1ef5c9
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ============================================
   2: # 4.1 Evaluate Engineered Features
   3: # ============================================
   4: print("Evaluating engineered features...")
   5: # Identify our engineered features
   6: engineered_features = [
   7:     'Total_SF', 'Total_Bathrooms', 'Total_Porch_SF', 'House_Age', 
   8:     'Years_Since_Remod', 'Has_Pool', 'Has_Garage', 'Has_Basement', 
   9:     'Has_Fireplace', 'Has_2nd_Floor', 'Quality_Area_Interaction',
  10:     'Lot Area_Log', 'Gr Liv Area_Log', 'Total_SF_Log'
  11: ]
  12: # Check which engineered features are still in the dataset (after correlation filtering)
  13: available_engineered = [f for f in engineered_features if f in df_reduced.columns]
  14: print(f"\nEngineered features still in dataset: {len(available_engineered)}")
  15: print("Features:")
  ... (15 more lines)

WHAT THIS CODE DOES:
  ‚Üí Creates scatter plots of top features vs SalePrice
  ‚Üí Visualizes bivariate relationships


========================================================================================================================
CELL #080 | Type: MARKDOWN | ID: 66187f39
========================================================================================================================
CONTENT:
  ---
  <a id='final-dataset'></a>
  
  ## 5. Final Dataset Preparation
  
  Preparing the final engineered dataset for modeling by:
  1. Removing temporary/analysis columns
  2. Saving the processed dataset
  3. Generating final summary statistics
  
  ---
  

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #081 | Type: CODE | ID: 8716a047
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # ============================================
   2: # 5.1 Prepare Final Dataset
   3: # ============================================
   4: print("Preparing final dataset for modeling...")
   5: # Remove temporary analysis columns
   6: columns_to_remove = ['Price_per_SF']  # This was only for analysis
   7: df_final = df_reduced.drop(columns=columns_to_remove, errors='ignore')
   8: # Remove identifier columns for modeling dataset
   9: modeling_cols_to_remove = ['Order', 'PID']
  10: df_modeling = df_final.drop(columns=modeling_cols_to_remove, errors='ignore')
  11: print(f"\n‚úÖ Final dataset prepared")
  12: print(f"Shape: {df_final.shape}")
  13: print(f"Modeling dataset shape (without identifiers): {df_modeling.shape}")
  14: # Summary statistics
  15: print("\nüìä Final Dataset Summary:")
  ... (16 more lines)

WHAT THIS CODE DOES:
  ‚Üí Verifies data schema
  ‚Üí Lists all column names and data types


========================================================================================================================
CELL #082 | Type: MARKDOWN | ID: 4a9b0e5a
========================================================================================================================
CONTENT:
  ---
  
  ## Phase 2C Summary
  
  ### ‚úÖ Deliverables Completed:
  
  #### 1. **Feature Creation & Transformation** ‚úÖ
  - Created 13 new features:
    - Aggregate features: Total_SF, Total_Bathrooms, Total_Porch_SF
    - Temporal features: House_Age, Years_Since_Remod
    - Binary indicators: Has_Pool, Has_Garage, Has_Basement, Has_Fireplace, Has_2nd_Floor
    - Interaction: Quality_Area_Interaction
    - Price analysis: Price_per_SF
  - Applied log transformations to 4 skewed features
  - Encoded 33 categorical variables (9 ordinal, 24 label encoded)
  
  #### 2. **Feature Selection & Dimensionality Reduction** ‚úÖ
  - **Correlation-based filtering**: Removed 16 highly correlated features (>0.85)
  - **Random Forest importance**: Identified top predictors
    - Overall Qual (61.7% importance)
  ... (30 more lines)

PURPOSE:
  ‚Üí Phase 3 section header or description


========================================================================================================================
CELL #083 | Type: MARKDOWN | ID: 604bc828
========================================================================================================================
CONTENT:
  ---
  
  ## Complete Phase 2 Summary
  
  ### üéØ **Overall Objective Achieved**
  
  Successfully completed comprehensive data preprocessing, exploratory data analysis, and feature engineering to prepare the Ames Housing dataset for predictive modeling.
  
  ---
  
  ### ‚úÖ **Phase 2A: Data Preprocessing & EDA - Deliverables**
  
  1. **Data Audit Completed**
     - Verified dataset structure: 2,930 rows √ó 82 columns
     - Identified 27 columns with missing values
     - Confirmed 0 duplicate rows
     - Identified 2 unique identifier columns (Order, PID)
  
  2. **Data Quality Assessment**
     - Created comprehensive metadata summary
  ... (93 more lines)

PURPOSE:
  ‚Üí Phase 2A section header or description


========================================================================================================================
CELL #084 | Type: MARKDOWN | ID: phase3-separator
========================================================================================================================
CONTENT:
  ---
  
  # PHASE 3: MODELING & INFERENCING
  
  ---

PURPOSE:
  ‚Üí Phase 3 section header or description


========================================================================================================================
CELL #085 | Type: MARKDOWN | ID: setup-header
========================================================================================================================
CONTENT:
  ## Step 1: Setup & Import Libraries
  
  ### üéØ What are we doing?
  Importing all necessary Python libraries for data manipulation, machine learning, and visualization.
  
  ### ü§î Why these libraries?
  - **pandas**: Load and work with datasets
  - **numpy**: Mathematical operations
  - **scikit-learn**: Build and evaluate machine learning models
  - **matplotlib/seaborn**: Create visualizations
  
  ### üìä Expected Result:
  All libraries should import successfully with no errors.
  
  ---

PURPOSE:
  ‚Üí Documentation for importing libraries


========================================================================================================================
CELL #086 | Type: CODE | ID: imports
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Import necessary libraries
   2: import pandas as pd
   3: import numpy as np
   4: import matplotlib.pyplot as plt
   5: import seaborn as sns
   6: # Machine Learning libraries
   7: from sklearn.model_selection import train_test_split
   8: from sklearn.linear_model import LinearRegression
   9: from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
  10: # Settings
  11: import warnings
  12: warnings.filterwarnings('ignore')
  13: # Display settings
  14: pd.set_option('display.max_columns', None)
  15: pd.set_option('display.float_format', '{:.2f}'.format)
  ... (6 more lines)

WHAT THIS CODE DOES:
  ‚Üí Imports required libraries (pandas, numpy, matplotlib, seaborn, warnings)


========================================================================================================================
CELL #087 | Type: MARKDOWN | ID: load-data-header
========================================================================================================================
CONTENT:
  ---
  
  ## Step 2: Load Engineered Dataset
  
  ### üéØ What are we doing?
  Loading the **engineered dataset** from Phase 2C (`AmesHousing_engineered.csv`).
  
  ### ü§î Why this specific dataset?
  This dataset contains:
  - ‚úÖ All missing values handled (imputed)
  - ‚úÖ All categorical features encoded (converted to numbers)
  - ‚úÖ New engineered features (Total_SF, Total_Bathrooms, etc.)
  - ‚úÖ Log-transformed skewed features for better modeling
  - ‚úÖ Multicollinearity reduced (highly correlated features removed)
  
  ### üìä Expected Result:
  - Dataset shape: **(2,930 rows √ó 71 columns)**
  - Target variable: `SalePrice` (house prices in dollars)
  - All features should be numeric (ready for machine learning)
  - Minimal to no missing values
  ... (2 more lines)

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #088 | Type: CODE | ID: load-data
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Load the engineered dataset from Phase 2C
   2: df = pd.read_csv("../data/AmesHousing_engineered.csv")
   3: print("‚úÖ Dataset loaded successfully!")
   4: print(f"\nüìä Dataset Shape: {df.shape}")
   5: print(f"   - Total Records (Houses): {df.shape[0]:,}")
   6: print(f"   - Total Features (Columns): {df.shape[1]}")
   7: # Check for missing values
   8: missing_count = df.isnull().sum().sum()
   9: print(f"\n‚ùì Missing Values: {missing_count}")
  10: if missing_count > 0:
  11:     print(f"   ‚ö†Ô∏è Warning: Found {missing_count} missing values - need to handle these")
  12: else:
  13:     print("   ‚úÖ No missing values - dataset is complete!")
  14: # Check data types
  15: print(f"\nüìã Data Types:")
  ... (4 more lines)

WHAT THIS CODE DOES:
  ‚Üí Loads the Ames Housing dataset from CSV file
  ‚Üí Displays dataset shape and first few rows


========================================================================================================================
CELL #089 | Type: MARKDOWN | ID: z9gzkxe635
========================================================================================================================
CONTENT:
  ---
  
  ## Step 3: Investigate Missing Values
  
  ### üéØ What are we doing?
  Identifying exactly WHERE the 3 missing values are located (which columns and rows).
  
  ### ü§î Why investigate first?
  Before handling missing values, we need to understand:
  - Which features contain missing data
  - How many missing values per feature
  - Whether these are critical features for modeling
  
  ### üìä Expected Result:
  - List of columns with missing values
  - Count of missing values per column
  - Decision on how to handle them (drop, impute, or investigate)
  
  ---

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #090 | Type: CODE | ID: 9gg25rq5onu
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Identify columns with missing values
   2: missing_by_column = df.isnull().sum()
   3: missing_features = missing_by_column[missing_by_column > 0]
   4: print("üîç Columns with Missing Values:")
   5: print("="*50)
   6: if len(missing_features) > 0:
   7:     for col, count in missing_features.items():
   8:         percentage = (count / len(df)) * 100
   9:         print(f"   ‚Ä¢ {col}: {count} missing ({percentage:.2f}%)")
  10:     print(f"\nüìä Total missing values: {missing_features.sum()}")
  11:     # Show the rows with missing values
  12:     print(f"\nüìã Rows containing missing values:")
  13:     missing_rows = df[df.isnull().any(axis=1)]
  14:     print(missing_rows[missing_features.index.tolist()].head(10))
  15: else:
  ... (4 more lines)

WHAT THIS CODE DOES:
  ‚Üí Analyzes missing values across all columns
  ‚Üí Calculates count and percentage of missing data


========================================================================================================================
CELL #091 | Type: MARKDOWN | ID: s3ajndq6dc
========================================================================================================================
CONTENT:
  ### üîé Analysis of Missing Values
  
  **Finding**: All 3 missing values are in `Lot Frontage` column
  
  **Context**:
  - `Lot Frontage` = Linear feet of street connected to property
  - Only 0.10% of data missing (3 out of 2,930 rows)
  - Dataset is 99.9986% complete
  
  **Next Step**: Investigate these 3 rows to understand their characteristics before deciding on handling strategy.
  
  ---

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #092 | Type: CODE | ID: aasdg0yg16b
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Get the 3 rows with missing Lot Frontage
   2: missing_indices = [2256, 2788, 2892]
   3: missing_details = df.loc[missing_indices]
   4: print("üìã Detailed view of rows with missing Lot Frontage:")
   5: print("="*70)
   6: # Show key identifying features
   7: key_features = ['Order', 'PID', 'Neighborhood', 'Lot Area', 'Lot Frontage', 
   8:                 'Gr Liv Area', 'Overall Qual', 'SalePrice']
   9: print(missing_details[key_features])
  10: # Get statistics for Lot Frontage to understand typical values
  11: print("\n\nüìä Lot Frontage Statistics (for context):")
  12: print("="*70)
  13: print(df['Lot Frontage'].describe())
  14: # Check neighborhoods of missing rows
  15: print("\n\nüèòÔ∏è Neighborhoods of rows with missing values:")
  ... (11 more lines)

WHAT THIS CODE DOES:
  ‚Üí Investigates remaining missing values
  ‚Üí Identifies specific rows with missing data


========================================================================================================================
CELL #093 | Type: MARKDOWN | ID: oa8lbu1gm7
========================================================================================================================
CONTENT:
  ### ‚ö†Ô∏è Issue Found: Neighborhood median calculation returned NaN
  
  **Problem**: Need to properly calculate median by excluding NaN values
  
  **Next**: Re-calculate neighborhood statistics correctly and determine best imputation strategy
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #094 | Type: CODE | ID: qgvkrrf2qog
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Properly investigate neighborhoods with missing values
   2: print("üîç Proper Neighborhood Analysis:")
   3: print("="*70)
   4: for idx in missing_indices:
   5:     neighborhood = df.loc[idx, 'Neighborhood']
   6:     lot_area = df.loc[idx, 'Lot Area']
   7:     # Get all properties in this neighborhood (excluding current row)
   8:     neighborhood_data = df[(df['Neighborhood'] == neighborhood) & (df.index != idx)]
   9:     # Count properties in neighborhood
  10:     total_in_neighborhood = len(neighborhood_data)
  11:     # Get Lot Frontage values (excluding NaN)
  12:     lot_frontage_values = neighborhood_data['Lot Frontage'].dropna()
  13:     count_with_frontage = len(lot_frontage_values)
  14:     # Calculate median (properly, excluding NaN)
  15:     if count_with_frontage > 0:
  ... (19 more lines)

WHAT THIS CODE DOES:
  ‚Üí Investigates remaining missing values
  ‚Üí Identifies specific rows with missing data


========================================================================================================================
CELL #095 | Type: MARKDOWN | ID: ssr7vzantl
========================================================================================================================
CONTENT:
  ---
  
  ## Step 4: Handle Missing Values
  
  ### üéØ What are we doing?
  Imputing the 3 missing `Lot Frontage` values using the **overall median** (70.00 feet).
  
  ### ü§î Why overall median instead of neighborhood median?
  - **Problem**: Neighborhoods 10 and 12 have NO other properties with Lot Frontage data
  - **Solution**: Use the overall dataset median as the most reasonable estimate
  - **Median vs Mean**: Using median (70.00) instead of mean (69.46) because median is more robust to outliers
  
  ### üìä Expected Result:
  - All 3 missing values filled with 70.00 feet
  - Zero missing values in dataset
  - Dataset ready for machine learning
  
  ---

PURPOSE:
  ‚Üí Explanation of missing value handling approach


========================================================================================================================
CELL #096 | Type: CODE | ID: l8ozp69gf9i
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Handle missing values by imputing with overall median
   2: print("üîß Imputing Missing Lot Frontage Values:")
   3: print("="*70)
   4: # Store original count of missing values
   5: original_missing = df['Lot Frontage'].isnull().sum()
   6: print(f"Missing values BEFORE imputation: {original_missing}")
   7: # Calculate overall median
   8: overall_median = df['Lot Frontage'].median()
   9: print(f"Imputation value (overall median): {overall_median:.2f} feet")
  10: # Show which rows will be imputed
  11: missing_indices = df[df['Lot Frontage'].isnull()].index.tolist()
  12: print(f"\nRows to be imputed: {missing_indices}")
  13: # Perform imputation
  14: df['Lot Frontage'].fillna(overall_median, inplace=True)
  15: # Verify imputation
  ... (12 more lines)

WHAT THIS CODE DOES:
  ‚Üí Analyzes missing values across all columns
  ‚Üí Calculates count and percentage of missing data


========================================================================================================================
CELL #097 | Type: MARKDOWN | ID: itkr23sbu8
========================================================================================================================
CONTENT:
  ---
  
  ## Step 5: Prepare Features and Target Variable
  
  ### üéØ What are we doing?
  Separating the dataset into **features (X)** and **target variable (y)** for modeling.
  
  ### ü§î Key Decisions:
  - **Target Variable**: Use `SalePrice` (original price in dollars), not `SalePrice_Log`
  - **Features**: All columns EXCEPT identifiers (`Order`, `PID`) and target variables
  - **Columns to Exclude**: Order, PID, SalePrice, SalePrice_Log, Lot Area_Log (redundant with Lot Area)
  
  ### üìä Expected Result:
  - **X**: Feature matrix with ~66 features (all numeric)
  - **y**: Target vector with 2,930 house prices
  - Ready for train-test split
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #098 | Type: CODE | ID: fl1n5ac7uss
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Prepare features and target variable
   2: print("üîß Preparing Features and Target Variable:")
   3: print("="*70)
   4: # Define columns to exclude (identifiers and target variables)
   5: columns_to_exclude = ['Order', 'PID', 'SalePrice', 'SalePrice_Log', 'Lot Area_Log']
   6: print(f"Columns to exclude: {columns_to_exclude}")
   7: # Create feature matrix X (all columns except excluded ones)
   8: X = df.drop(columns=columns_to_exclude)
   9: # Create target vector y (SalePrice in dollars)
  10: y = df['SalePrice']
  11: print(f"\n‚úÖ Features (X):")
  12: print(f"   - Shape: {X.shape}")
  13: print(f"   - Total features: {X.shape[1]}")
  14: print(f"   - Total records: {X.shape[0]:,}")
  15: print(f"\n‚úÖ Target (y):")
  ... (15 more lines)

WHAT THIS CODE DOES:
  ‚Üí Analyzes missing values across all columns
  ‚Üí Calculates count and percentage of missing data


========================================================================================================================
CELL #099 | Type: MARKDOWN | ID: s5aqhui7mw9
========================================================================================================================
CONTENT:
  ---
  
  ## Step 6: Train-Test Split
  
  ### üéØ What are we doing?
  Splitting the dataset into **training set** (80%) and **testing set** (20%).
  
  ### ü§î Why split the data?
  - **Training set**: Used to build/train our regression models
  - **Testing set**: Used to evaluate model performance on unseen data
  - **80-20 split**: Standard practice that balances training data volume with testing reliability
  - **random_state=42**: Ensures reproducibility (same split every time we run)
  
  ### üìä Expected Result:
  - X_train: 2,344 samples (80%)
  - X_test: 586 samples (20%)
  - y_train: 2,344 prices
  - y_test: 586 prices
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #100 | Type: CODE | ID: r9d302fc83
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Split data into training and testing sets
   2: print("üîß Splitting Data into Train and Test Sets:")
   3: print("="*70)
   4: # Perform train-test split (80-20)
   5: X_train, X_test, y_train, y_test = train_test_split(
   6:     X, y, 
   7:     test_size=0.20,      # 20% for testing
   8:     random_state=42      # For reproducibility
   9: )
  10: print(f"‚úÖ Training Set:")
  11: print(f"   - X_train shape: {X_train.shape}")
  12: print(f"   - y_train shape: {y_train.shape}")
  13: print(f"   - Number of samples: {X_train.shape[0]:,} ({(X_train.shape[0]/len(X))*100:.1f}%)")
  14: print(f"   - Number of features: {X_train.shape[1]}")
  15: print(f"\n‚úÖ Testing Set:")
  ... (16 more lines)

WHAT THIS CODE DOES:
  ‚Üí Splits data into training (80%) and testing (20%) sets
  ‚Üí Sets random_state=42 for reproducibility


========================================================================================================================
CELL #101 | Type: MARKDOWN | ID: igs6guxmy2r
========================================================================================================================
CONTENT:
  ---
  
  ## Step 7: Simple Linear Regression Model
  
  ### üéØ What are we doing?
  Building a **Simple Linear Regression** model using the **ONE best feature** to predict house prices.
  
  ### ü§î Why Simple Linear Regression first?
  - Establishes a **baseline** for model performance
  - Shows how much predictive power a single feature has
  - Easy to interpret and visualize
  - From Phase 2, we know `Overall Qual` has the strongest correlation (0.80)
  
  ### üìä Expected Result:
  - Identify the best single feature (highest correlation with SalePrice)
  - Build Simple LR model using only that feature
  - Expected R¬≤ ‚âà 0.64 (correlation¬≤ = 0.80¬≤ ‚âà 0.64)
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #102 | Type: CODE | ID: qb6u0k2fzq
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Step 7a: Identify the best single feature
   2: print("üîç Identifying the Best Single Feature for Simple Linear Regression:")
   3: print("="*70)
   4: # Calculate correlations between all features and target variable
   5: correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)
   6: print(f"üìä Top 10 Features by Correlation with SalePrice:")
   7: print("="*70)
   8: for i, (feature, corr) in enumerate(correlations.head(10).items(), 1):
   9:     print(f"   {i:2d}. {feature:25s} : {corr:.4f}")
  10: # Select the best feature
  11: best_feature = correlations.index[0]
  12: best_correlation = correlations.iloc[0]
  13: print(f"\n‚úÖ Best Single Feature Selected:")
  14: print(f"   Feature: {best_feature}")
  15: print(f"   Correlation: {best_correlation:.4f}")
  ... (2 more lines)

WHAT THIS CODE DOES:
  ‚Üí Analyzes correlations between features
  ‚Üí Identifies highly correlated features


========================================================================================================================
CELL #103 | Type: MARKDOWN | ID: aje7dq6nt6u
========================================================================================================================
CONTENT:
  ### üìà Step 7b: Build and Train Simple Linear Regression
  
  **What we're doing**: Training the model using only `Overall Qual` to predict `SalePrice`
  
  **Model equation**: `SalePrice = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó Overall_Qual`
  
  Where:
  - Œ≤‚ÇÄ = intercept (base price)
  - Œ≤‚ÇÅ = coefficient (price increase per quality point)
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #104 | Type: CODE | ID: 9m3nr1docpp
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Step 7b: Build and train Simple Linear Regression model
   2: print("üèóÔ∏è Building Simple Linear Regression Model:")
   3: print("="*70)
   4: # Prepare data with only the best feature (Overall Qual)
   5: X_train_simple = X_train[[best_feature]]  # Must be 2D array for sklearn
   6: X_test_simple = X_test[[best_feature]]
   7: print(f"Training data shape: {X_train_simple.shape}")
   8: print(f"Testing data shape: {X_test_simple.shape}")
   9: # Create and train the model
  10: simple_lr = LinearRegression()
  11: simple_lr.fit(X_train_simple, y_train)
  12: print(f"\n‚úÖ Model Trained Successfully!")
  13: print(f"\nüìä Model Parameters:")
  14: print(f"   Intercept (Œ≤‚ÇÄ): ${simple_lr.intercept_:,.2f}")
  15: print(f"   Coefficient (Œ≤‚ÇÅ) for {best_feature}: ${simple_lr.coef_[0]:,.2f}")
  ... (9 more lines)

WHAT THIS CODE DOES:
  ‚Üí Builds and trains Simple Linear Regression model
  ‚Üí Uses only the best single feature (Overall Qual)


========================================================================================================================
CELL #105 | Type: MARKDOWN | ID: l2l8lcf4oi
========================================================================================================================
CONTENT:
  ### üìä Step 7c: Evaluate Simple Linear Regression Performance
  
  **What we're evaluating**:
  - **R¬≤ Score**: How much variance in price our model explains (0 to 1)
  - **RMSE**: Average prediction error in dollars (lower is better)
  - **MAE**: Average absolute error in dollars (lower is better)
  
  **Evaluation on**: Both training set (to check fit) and testing set (to check generalization)
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #106 | Type: CODE | ID: dpawz2bdsbh
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Step 7c: Evaluate Simple Linear Regression model
   2: print("üìä Evaluating Simple Linear Regression Model:")
   3: print("="*70)
   4: # Make predictions on training set
   5: y_train_pred_simple = simple_lr.predict(X_train_simple)
   6: # Make predictions on testing set
   7: y_test_pred_simple = simple_lr.predict(X_test_simple)
   8: # Calculate metrics for TRAINING set
   9: train_r2_simple = r2_score(y_train, y_train_pred_simple)
  10: train_rmse_simple = np.sqrt(mean_squared_error(y_train, y_train_pred_simple))
  11: train_mae_simple = mean_absolute_error(y_train, y_train_pred_simple)
  12: # Calculate metrics for TESTING set
  13: test_r2_simple = r2_score(y_test, y_test_pred_simple)
  14: test_rmse_simple = np.sqrt(mean_squared_error(y_test, y_test_pred_simple))
  15: test_mae_simple = mean_absolute_error(y_test, y_test_pred_simple)
  ... (21 more lines)

WHAT THIS CODE DOES:
  ‚Üí Evaluates Simple Linear Regression performance
  ‚Üí Calculates R¬≤, RMSE, MAE on train and test sets


========================================================================================================================
CELL #107 | Type: MARKDOWN | ID: msh4veldt8
========================================================================================================================
CONTENT:
  ---
  
  ## Step 8: Multiple Linear Regression Model
  
  ### üéØ What are we doing?
  Building a **Multiple Linear Regression** model using **ALL 66 features** to predict house prices.
  
  ### ü§î Why Multiple Linear Regression?
  - Uses **ALL available information** (66 features vs just 1)
  - Captures complex relationships between multiple factors
  - Expected to perform better than Simple LR
  - More realistic for real-world predictions
  
  ### üìä Expected Result:
  - Use all 66 features simultaneously
  - Expected R¬≤ > 0.70 (better than Simple LR's 0.65)
  - Lower prediction error (MAE)
  - More accurate price predictions
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #108 | Type: CODE | ID: rppehytx7cs
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Step 8a: Build and train Multiple Linear Regression model
   2: print("üèóÔ∏è Building Multiple Linear Regression Model:")
   3: print("="*70)
   4: print(f"Using ALL features:")
   5: print(f"   Training data shape: {X_train.shape}")
   6: print(f"   Testing data shape: {X_test.shape}")
   7: print(f"   Total features: {X_train.shape[1]}")
   8: # Create and train the model
   9: multiple_lr = LinearRegression()
  10: multiple_lr.fit(X_train, y_train)
  11: print(f"\n‚úÖ Model Trained Successfully!")
  12: print(f"\nüìä Model Parameters:")
  13: print(f"   Intercept (Œ≤‚ÇÄ): ${multiple_lr.intercept_:,.2f}")
  14: print(f"   Number of coefficients: {len(multiple_lr.coef_)}")
  15: # Show top 5 most influential features (by absolute coefficient value)
  ... (11 more lines)

WHAT THIS CODE DOES:
  ‚Üí Builds and trains Multiple Linear Regression model
  ‚Üí Uses all available features


========================================================================================================================
CELL #109 | Type: MARKDOWN | ID: 9z7lig5ibzc
========================================================================================================================
CONTENT:
  ### üìä Step 8b: Evaluate Multiple Linear Regression Performance
  
  **Evaluating**: Same metrics as Simple LR for direct comparison
  - R¬≤ Score (variance explained)
  - RMSE (root mean squared error)
  - MAE (mean absolute error)
  
  **Goal**: Multiple LR should outperform Simple LR by using all available features
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #110 | Type: CODE | ID: 7y1u3z01fc
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Step 8b: Evaluate Multiple Linear Regression model
   2: print("üìä Evaluating Multiple Linear Regression Model:")
   3: print("="*70)
   4: # Make predictions on training set
   5: y_train_pred_multiple = multiple_lr.predict(X_train)
   6: # Make predictions on testing set
   7: y_test_pred_multiple = multiple_lr.predict(X_test)
   8: # Calculate metrics for TRAINING set
   9: train_r2_multiple = r2_score(y_train, y_train_pred_multiple)
  10: train_rmse_multiple = np.sqrt(mean_squared_error(y_train, y_train_pred_multiple))
  11: train_mae_multiple = mean_absolute_error(y_train, y_train_pred_multiple)
  12: # Calculate metrics for TESTING set
  13: test_r2_multiple = r2_score(y_test, y_test_pred_multiple)
  14: test_rmse_multiple = np.sqrt(mean_squared_error(y_test, y_test_pred_multiple))
  15: test_mae_multiple = mean_absolute_error(y_test, y_test_pred_multiple)
  ... (21 more lines)

WHAT THIS CODE DOES:
  ‚Üí Evaluates Multiple Linear Regression performance
  ‚Üí Calculates R¬≤, RMSE, MAE on train and test sets


========================================================================================================================
CELL #111 | Type: MARKDOWN | ID: iozv3spqsz
========================================================================================================================
CONTENT:
  ---
  
  ## Step 9: Model Comparison & Final Results
  
  ### üéØ What are we comparing?
  Side-by-side comparison of **Simple LR** vs **Multiple LR** performance
  
  ### üìä Key Questions:
  1. How much does using all features improve performance?
  2. Which model generalizes better to unseen data?
  3. What's the practical impact on prediction accuracy?
  
  ---

PURPOSE:
  ‚Üí Documentation/explanation for the following code section


========================================================================================================================
CELL #112 | Type: CODE | ID: 7x7d1jbnb8e
========================================================================================================================
CODE PREVIEW (first 15 lines):
   1: # Step 9: Model Comparison
   2: print("üìä MODEL COMPARISON: Simple LR vs Multiple LR")
   3: print("="*80)
   4: # Create comparison dataframe
   5: comparison = pd.DataFrame({
   6:     'Metric': ['Features Used', 'R¬≤ (Train)', 'R¬≤ (Test)', 'RMSE (Test)', 'MAE (Test)', 'R¬≤ Difference'],
   7:     'Simple LR': [
   8:         f'1 ({best_feature})',
   9:         f'{train_r2_simple:.4f}',
  10:         f'{test_r2_simple:.4f}',
  11:         f'${test_rmse_simple:,.0f}',
  12:         f'${test_mae_simple:,.0f}',
  13:         f'{abs(train_r2_simple - test_r2_simple):.4f}'
  14:     ],
  15:     'Multiple LR': [
  ... (37 more lines)

WHAT THIS CODE DOES:
  ‚Üí Compares Simple LR vs Multiple LR models
  ‚Üí Creates comparison table and visualizations
  ‚Üí Shows performance metrics side-by-side


========================================================================================================================
END OF CELL-BY-CELL ANALYSIS
========================================================================================================================
