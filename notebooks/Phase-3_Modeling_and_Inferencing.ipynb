{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 3: Modeling & Inferencing\n",
    "\n",
    "**Team**: The Outliers  \n",
    "**Course**: Advanced Apex Project 1 - BITS Pilani Digital  \n",
    "**Phase**: 3 (Model Construction & Evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Table of Contents\n",
    "\n",
    "1. [Introduction & Objectives](#introduction)\n",
    "2. [Setup & Data Loading](#setup)\n",
    "3. [Data Preparation for Modeling](#preparation)\n",
    "4. [Model 1: Simple Linear Regression](#simple-lr)\n",
    "5. [Model 2: Multiple Linear Regression](#multiple-lr)\n",
    "6. [Model Comparison & Evaluation](#comparison)\n",
    "7. [Insights & Conclusions](#conclusions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "## 1. Introduction & Objectives\n",
    "\n",
    "### ğŸ¯ What are we doing in this phase?\n",
    "\n",
    "In Phase 3, we will build **regression models** to predict house prices (`SalePrice`) based on the features we engineered in Phase 2. This is the core machine learning component where we transform our cleaned and engineered data into predictive models.\n",
    "\n",
    "### ğŸ¤” Why are we building these models?\n",
    "\n",
    "The goal is to:\n",
    "- **Predict house prices accurately** for properties in Ames, Iowa\n",
    "- **Understand which features** most strongly influence property values\n",
    "- **Compare different modeling approaches** to find the best predictor\n",
    "- **Evaluate model performance** using statistical metrics\n",
    "\n",
    "### ğŸ“Š What models will we build?\n",
    "\n",
    "As per Phase 3 requirements, we will implement:\n",
    "\n",
    "1. **Simple Linear Regression**: Uses ONE feature (the strongest predictor) to predict price\n",
    "2. **Multiple Linear Regression**: Uses ALL relevant features to predict price\n",
    "\n",
    "### ğŸ“ How will we evaluate success?\n",
    "\n",
    "We will use three key metrics:\n",
    "\n",
    "| Metric | What it Measures | Goal |\n",
    "|--------|------------------|------|\n",
    "| **RÂ² (R-squared)** | How much variance in price is explained by our model | **Higher is better** (closer to 1.0) |\n",
    "| **RMSE (Root Mean Squared Error)** | Average prediction error in dollars | **Lower is better** (closer to 0) |\n",
    "| **MAE (Mean Absolute Error)** | Average absolute difference between predicted and actual price | **Lower is better** (closer to 0) |\n",
    "\n",
    "### ğŸ”® Expected Outcomes\n",
    "\n",
    "Based on Phase 2 EDA:\n",
    "- **Overall Quality** had the strongest correlation (0.80) â†’ should be excellent for Simple Linear Regression\n",
    "- Multiple Linear Regression should outperform Simple Linear Regression by leveraging multiple predictors\n",
    "- We expect RÂ² > 0.70 for Multiple Linear Regression based on our feature correlations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "\n",
    "## 2. Setup & Data Loading\n",
    "\n",
    "### ğŸ¯ What are we doing?\n",
    "\n",
    "We'll import necessary libraries and load the **engineered dataset** from Phase 2C.\n",
    "\n",
    "### ğŸ¤” Why these specific libraries?\n",
    "\n",
    "- **pandas**: Load and manipulate data\n",
    "- **numpy**: Mathematical operations\n",
    "- **scikit-learn**: Machine learning models and evaluation tools\n",
    "- **matplotlib/seaborn**: Visualize results\n",
    "\n",
    "### ğŸ“Š Expected Result\n",
    "\n",
    "- Dataset shape: **(2930 rows Ã— 71 columns)**\n",
    "- All features encoded and ready for modeling\n",
    "- No missing values (except the 3 we identified, which we'll handle)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ“¦ Pandas version: {pd.__version__}\")\n",
    "print(f\"ğŸ“¦ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Load Engineered Dataset\n",
    "\n",
    "We'll load the dataset we prepared in Phase 2C (`AmesHousing_engineered.csv`).\n",
    "\n",
    "**Why this dataset?**  \n",
    "This version includes:\n",
    "- âœ… All missing values handled\n",
    "- âœ… All categorical features encoded\n",
    "- âœ… New engineered features (Total_SF, Total_Bathrooms, etc.)\n",
    "- âœ… Log-transformed skewed features\n",
    "- âœ… Multicollinearity reduced\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the engineered dataset from Phase 2C\n",
    "df = pd.read_csv(\"../data/AmesHousing_engineered.csv\")\n",
    "\n",
    "print(\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"\\nğŸ“Š Dataset Shape: {df.shape}\")\n",
    "print(f\"   - Total Records: {df.shape[0]:,}\")\n",
    "print(f\"   - Total Features: {df.shape[1]}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df.isnull().sum().sum()\n",
    "print(f\"\\nâ“ Missing Values: {missing_count}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nğŸ“‹ First 3 rows of the dataset:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-info-header",
   "metadata": {},
   "source": [
    "### ğŸ” Quick Data Inspection\n",
    "\n",
    "Let's verify our target variable and understand the data types.\n",
    "\n",
    "**What we're checking:**\n",
    "- Target variable: `SalePrice` (what we want to predict)\n",
    "- Features: All other columns that will help predict the price\n",
    "- Data types: Ensure all are numeric (required for regression models)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names and data types\n",
    "print(\"ğŸ“Š Dataset Information:\")\n",
    "print(\"\\nğŸ¯ Target Variable: SalePrice\")\n",
    "print(f\"   - Data Type: {df['SalePrice'].dtype}\")\n",
    "print(f\"   - Min Price: ${df['SalePrice'].min():,.0f}\")\n",
    "print(f\"   - Max Price: ${df['SalePrice'].max():,.0f}\")\n",
    "print(f\"   - Mean Price: ${df['SalePrice'].mean():,.0f}\")\n",
    "print(f\"   - Median Price: ${df['SalePrice'].median():,.0f}\")\n",
    "\n",
    "# Data type summary\n",
    "print(\"\\nğŸ“‹ Data Types Summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check for any object (string) columns that shouldn't be there\n",
    "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if len(object_cols) > 0:\n",
    "    print(f\"\\nâš ï¸ Warning: Found {len(object_cols)} non-numeric columns: {object_cols}\")\n",
    "    print(\"   These need to be handled before modeling.\")\n",
    "else:\n",
    "    print(\"\\nâœ… All columns are numeric - ready for modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preparation-header",
   "metadata": {},
   "source": [
    "<a id='preparation'></a>\n",
    "\n",
    "## 3. Data Preparation for Modeling\n",
    "\n",
    "### ğŸ¯ What are we doing?\n",
    "\n",
    "Before training our models, we need to:\n",
    "1. **Handle remaining missing values** (if any)\n",
    "2. **Separate features (X) from target (y)**\n",
    "3. **Split data** into training set (80%) and testing set (20%)\n",
    "4. **Identify the best single feature** for Simple Linear Regression\n",
    "\n",
    "### ğŸ¤” Why split into train and test?\n",
    "\n",
    "- **Training Set**: Used to teach the model patterns (80% of data)\n",
    "- **Testing Set**: Used to evaluate model on unseen data (20% of data)\n",
    "- This prevents **overfitting** and gives us realistic performance estimates\n",
    "\n",
    "### ğŸ“Š Expected Result\n",
    "\n",
    "- Training set: ~2,344 houses (80%)\n",
    "- Testing set: ~586 houses (20%)\n",
    "- Both sets should have similar price distributions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handle-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle any remaining missing values\n",
    "print(\"ğŸ”§ Handling Missing Values...\\n\")\n",
    "\n",
    "# Check which columns have missing values\n",
    "missing_info = df.isnull().sum()\n",
    "missing_cols = missing_info[missing_info > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"Found {len(missing_cols)} columns with missing values:\")\n",
    "    print(missing_cols)\n",
    "    \n",
    "    # Fill missing values with median for numeric columns\n",
    "    for col in missing_cols.index:\n",
    "        if df[col].dtype in ['int64', 'int32', 'float64']:\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "            print(f\"   âœ… Filled {col} with median: {median_val:.2f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… All missing values handled!\")\n",
    "else:\n",
    "    print(\"âœ… No missing values found - data is complete!\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "final_missing = df.isnull().sum().sum()\n",
    "print(f\"\\nğŸ“Š Final missing value count: {final_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-features-header",
   "metadata": {},
   "source": [
    "### ğŸ¯ Separate Features and Target\n",
    "\n",
    "**What we're doing:**\n",
    "- Create **X** (features): All columns except `SalePrice`, `Order`, `PID`\n",
    "- Create **y** (target): Only the `SalePrice` column\n",
    "\n",
    "**Why exclude Order and PID?**\n",
    "- These are just identifiers, not predictive features\n",
    "- Including them would confuse the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns to exclude from features\n",
    "exclude_cols = ['Order', 'PID', 'SalePrice', 'SalePrice_Log']\n",
    "exclude_cols = [col for col in exclude_cols if col in df.columns]\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=exclude_cols)\n",
    "y = df['SalePrice']\n",
    "\n",
    "print(\"âœ… Features and target separated successfully!\\n\")\n",
    "print(f\"ğŸ“Š Feature Matrix (X) Shape: {X.shape}\")\n",
    "print(f\"   - Number of samples: {X.shape[0]:,}\")\n",
    "print(f\"   - Number of features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Target Variable (y) Shape: {y.shape}\")\n",
    "print(f\"   - Number of samples: {y.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Feature names ({len(X.columns)} features):\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "find-best-feature-header",
   "metadata": {},
   "source": [
    "### ğŸ” Identify Best Single Feature for Simple Linear Regression\n",
    "\n",
    "**What we're doing:**\n",
    "- Calculate correlation between each feature and SalePrice\n",
    "- Find the feature with the **highest absolute correlation**\n",
    "- This will be used in our Simple Linear Regression model\n",
    "\n",
    "**Why?**\n",
    "- Simple Linear Regression uses only ONE feature\n",
    "- We want to use the most predictive feature for best results\n",
    "- From Phase 2, we expect `Overall Qual` to be the strongest\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-best-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation of all features with SalePrice\n",
    "correlations = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Correlation': [X[col].corr(y) for col in X.columns]\n",
    "})\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlations['Abs_Correlation'] = correlations['Correlation'].abs()\n",
    "correlations = correlations.sort_values('Abs_Correlation', ascending=False)\n",
    "\n",
    "# Get the best feature\n",
    "best_feature = correlations.iloc[0]['Feature']\n",
    "best_correlation = correlations.iloc[0]['Correlation']\n",
    "\n",
    "print(\"ğŸ” Feature Correlation Analysis:\\n\")\n",
    "print(f\"ğŸ† Best Single Feature: {best_feature}\")\n",
    "print(f\"ğŸ“Š Correlation with SalePrice: {best_correlation:.4f}\")\n",
    "print(f\"\\nğŸ“‹ Top 10 Most Correlated Features:\\n\")\n",
    "print(correlations[['Feature', 'Correlation']].head(10).to_string(index=False))\n",
    "\n",
    "# Visualize top 15 correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_15 = correlations.head(15)\n",
    "plt.barh(range(len(top_15)), top_15['Correlation'].values, color='steelblue')\n",
    "plt.yticks(range(len(top_15)), top_15['Feature'].values)\n",
    "plt.xlabel('Correlation with SalePrice')\n",
    "plt.title('Top 15 Features by Correlation with House Price')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… We'll use '{best_feature}' for Simple Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-test-split-header",
   "metadata": {},
   "source": [
    "### âœ‚ï¸ Split Data into Training and Testing Sets\n",
    "\n",
    "**What we're doing:**\n",
    "- Split our data: 80% for training, 20% for testing\n",
    "- Use `random_state=42` for reproducibility\n",
    "\n",
    "**Why this split?**\n",
    "- 80/20 is a standard split in machine learning\n",
    "- Gives enough data to train while keeping enough to test\n",
    "- Random state ensures we get the same split each time we run this\n",
    "\n",
    "**Expected result:**\n",
    "- Training: ~2,344 houses\n",
    "- Testing: ~586 houses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"âœ‚ï¸ Data Split Complete!\\n\")\n",
    "print(\"ğŸ“Š Training Set:\")\n",
    "print(f\"   - X_train shape: {X_train.shape} ({X_train.shape[0]} houses, {X_train.shape[1]} features)\")\n",
    "print(f\"   - y_train shape: {y_train.shape} ({y_train.shape[0]} prices)\")\n",
    "print(f\"   - Percentage: {(len(X_train) / len(X)) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ“Š Testing Set:\")\n",
    "print(f\"   - X_test shape: {X_test.shape} ({X_test.shape[0]} houses, {X_test.shape[1]} features)\")\n",
    "print(f\"   - y_test shape: {y_test.shape} ({y_test.shape[0]} prices)\")\n",
    "print(f\"   - Percentage: {(len(X_test) / len(X)) * 100:.1f}%\")\n",
    "\n",
    "# Compare distributions\n",
    "print(\"\\nğŸ“ˆ Price Distribution Comparison:\")\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"   - Mean: ${y_train.mean():,.0f}\")\n",
    "print(f\"   - Median: ${y_train.median():,.0f}\")\n",
    "print(f\"   - Std Dev: ${y_train.std():,.0f}\")\n",
    "\n",
    "print(f\"\\nTesting Set:\")\n",
    "print(f\"   - Mean: ${y_test.mean():,.0f}\")\n",
    "print(f\"   - Median: ${y_test.median():,.0f}\")\n",
    "print(f\"   - Std Dev: ${y_test.std():,.0f}\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(y_train, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Training Set - SalePrice Distribution')\n",
    "axes[0].set_xlabel('Sale Price ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(y_train.mean(), color='red', linestyle='--', label=f'Mean: ${y_train.mean():,.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(y_test, bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Testing Set - SalePrice Distribution')\n",
    "axes[1].set_xlabel('Sale Price ($)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(y_test.mean(), color='red', linestyle='--', label=f'Mean: ${y_test.mean():,.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Both sets have similar distributions - good split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-lr-header",
   "metadata": {},
   "source": [
    "<a id='simple-lr'></a>\n",
    "\n",
    "## 4. Model 1: Simple Linear Regression\n",
    "\n",
    "### ğŸ¯ What is Simple Linear Regression?\n",
    "\n",
    "Simple Linear Regression is a statistical method that models the relationship between:\n",
    "- **One independent variable (feature)**: Our best predictor\n",
    "- **One dependent variable (target)**: SalePrice\n",
    "\n",
    "**The equation:** `SalePrice = Î²â‚€ + Î²â‚ Ã— Feature + Îµ`\n",
    "\n",
    "Where:\n",
    "- `Î²â‚€` = intercept (baseline price)\n",
    "- `Î²â‚` = slope (how much price changes per unit increase in feature)\n",
    "- `Îµ` = error term\n",
    "\n",
    "### ğŸ¤” Why start with Simple Linear Regression?\n",
    "\n",
    "- **Easy to interpret**: We can see exactly how one feature affects price\n",
    "- **Baseline model**: Gives us a performance benchmark\n",
    "- **Visual**: Can be plotted on a 2D graph\n",
    "\n",
    "### ğŸ“Š Expected Results\n",
    "\n",
    "Based on our Phase 2 analysis:\n",
    "- If using `Overall Qual` (correlation = 0.80)\n",
    "- Expected RÂ² â‰ˆ 0.64 (since RÂ² = correlationÂ²)\n",
    "- This means ~64% of price variation can be explained by quality alone\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-simple-lr-header",
   "metadata": {},
   "source": [
    "### ğŸš‚ Train Simple Linear Regression Model\n",
    "\n",
    "**What we're doing:**\n",
    "1. Extract the best single feature from training and test sets\n",
    "2. Create and train a Linear Regression model\n",
    "3. Make predictions on the test set\n",
    "\n",
    "**Why reshape data?**\n",
    "- sklearn expects 2D arrays (even for single feature)\n",
    "- `.reshape(-1, 1)` converts 1D array to 2D with one column\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-simple-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best feature for Simple Linear Regression\n",
    "X_train_simple = X_train[[best_feature]]\n",
    "X_test_simple = X_test[[best_feature]]\n",
    "\n",
    "print(f\"ğŸš‚ Training Simple Linear Regression Model...\\n\")\n",
    "print(f\"ğŸ“Š Using feature: {best_feature}\")\n",
    "print(f\"   Training data shape: {X_train_simple.shape}\")\n",
    "print(f\"   Testing data shape: {X_test_simple.shape}\\n\")\n",
    "\n",
    "# Create and train the model\n",
    "simple_lr_model = LinearRegression()\n",
    "simple_lr_model.fit(X_train_simple, y_train)\n",
    "\n",
    "print(\"âœ… Model trained successfully!\\n\")\n",
    "\n",
    "# Get model parameters\n",
    "intercept = simple_lr_model.intercept_\n",
    "coefficient = simple_lr_model.coef_[0]\n",
    "\n",
    "print(\"ğŸ“ Model Parameters:\")\n",
    "print(f\"   - Intercept (Î²â‚€): ${intercept:,.2f}\")\n",
    "print(f\"   - Coefficient (Î²â‚): ${coefficient:,.2f}\")\n",
    "print(f\"\\nğŸ“ Model Equation:\")\n",
    "print(f\"   SalePrice = ${intercept:,.2f} + ${coefficient:,.2f} Ã— {best_feature}\")\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "print(f\"   - For each 1-unit increase in {best_feature},\")\n",
    "print(f\"   - The house price increases by ${coefficient:,.2f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_simple_train = simple_lr_model.predict(X_train_simple)\n",
    "y_pred_simple_test = simple_lr_model.predict(X_test_simple)\n",
    "\n",
    "print(f\"\\nâœ… Predictions generated for both training and testing sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-simple-lr-header",
   "metadata": {},
   "source": [
    "### ğŸ“Š Evaluate Simple Linear Regression Model\n",
    "\n",
    "**What we're doing:**\n",
    "- Calculate performance metrics on both training and test sets\n",
    "- Compare training vs. test performance to check for overfitting\n",
    "\n",
    "**What to look for:**\n",
    "- **RÂ² close to 1.0** = Model explains variance well\n",
    "- **Low RMSE and MAE** = Small prediction errors\n",
    "- **Training and test scores similar** = Model generalizes well (not overfitting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-simple-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for TRAINING set\n",
    "r2_train_simple = r2_score(y_train, y_pred_simple_train)\n",
    "rmse_train_simple = np.sqrt(mean_squared_error(y_train, y_pred_simple_train))\n",
    "mae_train_simple = mean_absolute_error(y_train, y_pred_simple_train)\n",
    "\n",
    "# Calculate metrics for TESTING set\n",
    "r2_test_simple = r2_score(y_test, y_pred_simple_test)\n",
    "rmse_test_simple = np.sqrt(mean_squared_error(y_test, y_pred_simple_test))\n",
    "mae_test_simple = mean_absolute_error(y_test, y_pred_simple_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š SIMPLE LINEAR REGRESSION - MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ¯ Feature Used: {best_feature}\\n\")\n",
    "\n",
    "print(\"ğŸ“ˆ TRAINING SET PERFORMANCE:\")\n",
    "print(f\"   RÂ² Score:  {r2_train_simple:.4f}  (Higher is better, max = 1.0)\")\n",
    "print(f\"   RMSE:      ${rmse_train_simple:,.2f}  (Lower is better)\")\n",
    "print(f\"   MAE:       ${mae_train_simple:,.2f}  (Lower is better)\")\n",
    "\n",
    "print(\"\\nğŸ“‰ TESTING SET PERFORMANCE:\")\n",
    "print(f\"   RÂ² Score:  {r2_test_simple:.4f}  (Higher is better, max = 1.0)\")\n",
    "print(f\"   RMSE:      ${rmse_test_simple:,.2f}  (Lower is better)\")\n",
    "print(f\"   MAE:       ${mae_test_simple:,.2f}  (Lower is better)\")\n",
    "\n",
    "print(\"\\nğŸ” MODEL ANALYSIS:\")\n",
    "print(f\"   - Training RÂ²: {r2_train_simple:.4f}\")\n",
    "print(f\"   - Testing RÂ²:  {r2_test_simple:.4f}\")\n",
    "print(f\"   - Difference:  {abs(r2_train_simple - r2_test_simple):.4f}\")\n",
    "\n",
    "if abs(r2_train_simple - r2_test_simple) < 0.05:\n",
    "    print(\"   âœ… Model generalizes well (no overfitting)\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Some overfitting detected (train vs test gap > 0.05)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "print(f\"   - The model explains {r2_test_simple*100:.2f}% of price variance\")\n",
    "print(f\"   - Average prediction error: ${mae_test_simple:,.2f}\")\n",
    "print(f\"   - For a house worth ${y_test.mean():,.0f}, error is {(mae_test_simple/y_test.mean())*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-simple-lr-header",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Visualize Simple Linear Regression Results\n",
    "\n",
    "**What we're visualizing:**\n",
    "1. **Regression Line**: Shows the linear relationship between feature and price\n",
    "2. **Predicted vs Actual**: How well predictions match reality\n",
    "3. **Residuals**: Prediction errors (should be randomly distributed)\n",
    "\n",
    "**Why visualize?**\n",
    "- Helps us understand model behavior\n",
    "- Identifies patterns in errors\n",
    "- Makes results interpretable for non-technical audiences\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-simple-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Regression Line\n",
    "axes[0].scatter(X_test_simple, y_test, alpha=0.5, label='Actual Prices')\n",
    "axes[0].plot(X_test_simple, y_pred_simple_test, color='red', linewidth=2, label='Regression Line')\n",
    "axes[0].set_xlabel(best_feature)\n",
    "axes[0].set_ylabel('Sale Price ($)')\n",
    "axes[0].set_title(f'Simple Linear Regression\\n{best_feature} vs SalePrice')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predicted vs Actual\n",
    "axes[1].scatter(y_test, y_pred_simple_test, alpha=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Price ($)')\n",
    "axes[1].set_ylabel('Predicted Price ($)')\n",
    "axes[1].set_title(f'Predicted vs Actual Prices\\nRÂ² = {r2_test_simple:.4f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals (errors)\n",
    "residuals_simple = y_test - y_pred_simple_test\n",
    "axes[2].scatter(y_pred_simple_test, residuals_simple, alpha=0.5)\n",
    "axes[2].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[2].set_xlabel('Predicted Price ($)')\n",
    "axes[2].set_ylabel('Residuals ($)')\n",
    "axes[2].set_title(f'Residual Plot\\nMAE = ${mae_test_simple:,.0f}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Visual Analysis:\")\n",
    "print(\"\\n1ï¸âƒ£ Regression Line Plot:\")\n",
    "print(\"   - Shows the linear relationship between feature and price\")\n",
    "print(\"   - Points close to line = good predictions\")\n",
    "print(\"   - Points far from line = larger errors\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Predicted vs Actual Plot:\")\n",
    "print(\"   - Points on red dashed line = perfect predictions\")\n",
    "print(\"   - Scatter around line shows prediction accuracy\")\n",
    "print(\"   - Tighter clustering = better model\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Residual Plot:\")\n",
    "print(\"   - Shows prediction errors\")\n",
    "print(\"   - Should be randomly scattered around zero\")\n",
    "print(\"   - Patterns indicate model limitations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-lr-header",
   "metadata": {},
   "source": [
    "<a id='multiple-lr'></a>\n",
    "\n",
    "## 5. Model 2: Multiple Linear Regression\n",
    "\n",
    "### ğŸ¯ What is Multiple Linear Regression?\n",
    "\n",
    "Multiple Linear Regression extends simple regression by using **multiple features** to predict the target.\n",
    "\n",
    "**The equation:** `SalePrice = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... + Î²â‚™Xâ‚™ + Îµ`\n",
    "\n",
    "Where:\n",
    "- `Î²â‚€` = intercept\n",
    "- `Î²â‚, Î²â‚‚, ..., Î²â‚™` = coefficients for each feature\n",
    "- `Xâ‚, Xâ‚‚, ..., Xâ‚™` = all our features\n",
    "- `Îµ` = error term\n",
    "\n",
    "### ğŸ¤” Why Multiple Linear Regression?\n",
    "\n",
    "- **More information**: Uses all available features\n",
    "- **Better predictions**: Captures complex relationships\n",
    "- **Real-world accuracy**: Houses are valued on multiple factors, not just one\n",
    "\n",
    "### ğŸ“Š Expected Results\n",
    "\n",
    "Multiple Linear Regression should **outperform** Simple Linear Regression:\n",
    "- Expected RÂ² > 0.70 (vs ~0.64 for simple)\n",
    "- Lower RMSE and MAE\n",
    "- Better captures price variation\n",
    "\n",
    "**Why?**\n",
    "- Uses all engineered features (Total_SF, Total_Bathrooms, etc.)\n",
    "- Combines information from quality, size, age, and location\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-multiple-lr-header",
   "metadata": {},
   "source": [
    "### ğŸš‚ Train Multiple Linear Regression Model\n",
    "\n",
    "**What we're doing:**\n",
    "1. Use ALL features in X_train and X_test\n",
    "2. Train a Multiple Linear Regression model\n",
    "3. Make predictions on test set\n",
    "\n",
    "**Key difference from Simple LR:**\n",
    "- Uses all " + str(len(X.columns)) + " features instead of just 1\n",
    "- Model learns optimal weights for each feature\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-multiple-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ğŸš‚ Training Multiple Linear Regression Model...\\n\")\n",
    "print(f\"ğŸ“Š Using ALL features: {X_train.shape[1]} features\")\n",
    "print(f\"   Training data shape: {X_train.shape}\")\n",
    "print(f\"   Testing data shape: {X_test.shape}\\n\")\n",
    "\n",
    "# Create and train the model\n",
    "multiple_lr_model = LinearRegression()\n",
    "multiple_lr_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Model trained successfully!\\n\")\n",
    "\n",
    "# Get model parameters\n",
    "intercept_multi = multiple_lr_model.intercept_\n",
    "coefficients_multi = multiple_lr_model.coef_\n",
    "\n",
    "print(\"ğŸ“ Model Parameters:\")\n",
    "print(f\"   - Intercept (Î²â‚€): ${intercept_multi:,.2f}\")\n",
    "print(f\"   - Number of coefficients: {len(coefficients_multi)}\")\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': coefficients_multi,\n",
    "    'Abs_Coefficient': np.abs(coefficients_multi)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ† Top 10 Most Important Features (by coefficient magnitude):\\n\")\n",
    "print(feature_importance[['Feature', 'Coefficient']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interpretation:\")\n",
    "print(f\"   - Positive coefficients â†’ feature increases price\")\n",
    "print(f\"   - Negative coefficients â†’ feature decreases price\")\n",
    "print(f\"   - Larger magnitude â†’ stronger effect on price\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_multi_train = multiple_lr_model.predict(X_train)\n",
    "y_pred_multi_test = multiple_lr_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nâœ… Predictions generated for both training and testing sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-multiple-lr-header",
   "metadata": {},
   "source": [
    "### ğŸ“Š Evaluate Multiple Linear Regression Model\n",
    "\n",
    "**What we're doing:**\n",
    "- Calculate the same metrics as Simple LR\n",
    "- Compare performance between the two models\n",
    "\n",
    "**Expected outcome:**\n",
    "- Multiple LR should have higher RÂ² and lower errors\n",
    "- If not, might indicate overfitting or poor feature selection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-multiple-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for TRAINING set\n",
    "r2_train_multi = r2_score(y_train, y_pred_multi_train)\n",
    "rmse_train_multi = np.sqrt(mean_squared_error(y_train, y_pred_multi_train))\n",
    "mae_train_multi = mean_absolute_error(y_train, y_pred_multi_train)\n",
    "\n",
    "# Calculate metrics for TESTING set\n",
    "r2_test_multi = r2_score(y_test, y_pred_multi_test)\n",
    "rmse_test_multi = np.sqrt(mean_squared_error(y_test, y_pred_multi_test))\n",
    "mae_test_multi = mean_absolute_error(y_test, y_pred_multi_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š MULTIPLE LINEAR REGRESSION - MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ¯ Features Used: All {X_train.shape[1]} features\\n\")\n",
    "\n",
    "print(\"ğŸ“ˆ TRAINING SET PERFORMANCE:\")\n",
    "print(f\"   RÂ² Score:  {r2_train_multi:.4f}  (Higher is better, max = 1.0)\")\n",
    "print(f\"   RMSE:      ${rmse_train_multi:,.2f}  (Lower is better)\")\n",
    "print(f\"   MAE:       ${mae_train_multi:,.2f}  (Lower is better)\")\n",
    "\n",
    "print(\"\\nğŸ“‰ TESTING SET PERFORMANCE:\")\n",
    "print(f\"   RÂ² Score:  {r2_test_multi:.4f}  (Higher is better, max = 1.0)\")\n",
    "print(f\"   RMSE:      ${rmse_test_multi:,.2f}  (Lower is better)\")\n",
    "print(f\"   MAE:       ${mae_test_multi:,.2f}  (Lower is better)\")\n",
    "\n",
    "print(\"\\nğŸ” MODEL ANALYSIS:\")\n",
    "print(f\"   - Training RÂ²: {r2_train_multi:.4f}\")\n",
    "print(f\"   - Testing RÂ²:  {r2_test_multi:.4f}\")\n",
    "print(f\"   - Difference:  {abs(r2_train_multi - r2_test_multi):.4f}\")\n",
    "\n",
    "if abs(r2_train_multi - r2_test_multi) < 0.05:\n",
    "    print(\"   âœ… Model generalizes well (no overfitting)\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Some overfitting detected (train vs test gap > 0.05)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "print(f\"   - The model explains {r2_test_multi*100:.2f}% of price variance\")\n",
    "print(f\"   - Average prediction error: ${mae_test_multi:,.2f}\")\n",
    "print(f\"   - For a house worth ${y_test.mean():,.0f}, error is {(mae_test_multi/y_test.mean())*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-multiple-lr-header",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Visualize Multiple Linear Regression Results\n",
    "\n",
    "**What we're visualizing:**\n",
    "1. **Predicted vs Actual**: How well the model predicts prices\n",
    "2. **Residuals Distribution**: Are errors normally distributed?\n",
    "3. **Top Feature Coefficients**: Which features matter most?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-multiple-lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, y_pred_multi_test, alpha=0.5)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Price ($)')\n",
    "axes[0, 0].set_ylabel('Predicted Price ($)')\n",
    "axes[0, 0].set_title(f'Predicted vs Actual Prices\\nRÂ² = {r2_test_multi:.4f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals vs Predicted\n",
    "residuals_multi = y_test - y_pred_multi_test\n",
    "axes[0, 1].scatter(y_pred_multi_test, residuals_multi, alpha=0.5)\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Price ($)')\n",
    "axes[0, 1].set_ylabel('Residuals ($)')\n",
    "axes[0, 1].set_title(f'Residual Plot\\nMAE = ${mae_test_multi:,.0f}')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals Distribution\n",
    "axes[1, 0].hist(residuals_multi, bins=50, color='lightblue', edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Residuals ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Prediction Errors')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Top 15 Feature Coefficients\n",
    "top_15_features = feature_importance.head(15)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_15_features['Coefficient']]\n",
    "axes[1, 1].barh(range(len(top_15_features)), top_15_features['Coefficient'], color=colors)\n",
    "axes[1, 1].set_yticks(range(len(top_15_features)))\n",
    "axes[1, 1].set_yticklabels(top_15_features['Feature'])\n",
    "axes[1, 1].set_xlabel('Coefficient Value')\n",
    "axes[1, 1].set_title('Top 15 Feature Coefficients\\n(Green=Positive, Red=Negative)')\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1, 1].invert_yaxis()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Visual Analysis:\")\n",
    "print(\"\\n1ï¸âƒ£ Predicted vs Actual:\")\n",
    "print(\"   - Points closer to diagonal line = more accurate predictions\")\n",
    "print(\"   - Should be tighter than Simple LR scatter\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ Residual Plot:\")\n",
    "print(\"   - Random scatter around zero = good model\")\n",
    "print(\"   - Patterns or trends = model missing something\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ Residuals Distribution:\")\n",
    "print(\"   - Should be bell-shaped (normal distribution)\")\n",
    "print(\"   - Centered around zero\")\n",
    "print(\"   - Confirms model assumptions\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ Feature Coefficients:\")\n",
    "print(\"   - Green bars = features that increase price\")\n",
    "print(\"   - Red bars = features that decrease price\")\n",
    "print(\"   - Length = strength of effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "\n",
    "## 6. Model Comparison & Evaluation\n",
    "\n",
    "### ğŸ¯ What are we doing?\n",
    "\n",
    "Now we'll **compare both models side-by-side** to determine which performs better and understand the value of using multiple features.\n",
    "\n",
    "### ğŸ¤” Why compare?\n",
    "\n",
    "- Shows the improvement from using multiple features\n",
    "- Helps understand trade-offs between simplicity and accuracy\n",
    "- Validates our feature engineering efforts from Phase 2\n",
    "\n",
    "### ğŸ“Š What to expect?\n",
    "\n",
    "Multiple Linear Regression should:\n",
    "- Have **higher RÂ²** (explains more variance)\n",
    "- Have **lower RMSE and MAE** (smaller errors)\n",
    "- Be **more accurate** overall\n",
    "\n",
    "But Simple Linear Regression has advantages:\n",
    "- **Easier to interpret** (one feature relationship)\n",
    "- **Faster to compute**\n",
    "- **Good baseline** for understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['RÂ² Score', 'RMSE ($)', 'MAE ($)', 'Number of Features'],\n",
    "    'Simple LR (Train)': [\n",
    "        f\"{r2_train_simple:.4f}\",\n",
    "        f\"${rmse_train_simple:,.2f}\",\n",
    "        f\"${mae_train_simple:,.2f}\",\n",
    "        \"1\"\n",
    "    ],\n",
    "    'Simple LR (Test)': [\n",
    "        f\"{r2_test_simple:.4f}\",\n",
    "        f\"${rmse_test_simple:,.2f}\",\n",
    "        f\"${mae_test_simple:,.2f}\",\n",
    "        \"1\"\n",
    "    ],\n",
    "    'Multiple LR (Train)': [\n",
    "        f\"{r2_train_multi:.4f}\",\n",
    "        f\"${rmse_train_multi:,.2f}\",\n",
    "        f\"${mae_train_multi:,.2f}\",\n",
    "        f\"{X_train.shape[1]}\"\n",
    "    ],\n",
    "    'Multiple LR (Test)': [\n",
    "        f\"{r2_test_multi:.4f}\",\n",
    "        f\"${rmse_test_multi:,.2f}\",\n",
    "        f\"${mae_test_multi:,.2f}\",\n",
    "        f\"{X_train.shape[1]}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Calculate improvements\n",
    "r2_improvement = ((r2_test_multi - r2_test_simple) / r2_test_simple) * 100\n",
    "rmse_improvement = ((rmse_test_simple - rmse_test_multi) / rmse_test_simple) * 100\n",
    "mae_improvement = ((mae_test_simple - mae_test_multi) / mae_test_simple) * 100\n",
    "\n",
    "print(\"\\nğŸ“ˆ IMPROVEMENT FROM SIMPLE TO MULTIPLE LINEAR REGRESSION:\\n\")\n",
    "print(f\"   RÂ² Score Improvement:  {r2_improvement:+.2f}%\")\n",
    "print(f\"   RMSE Reduction:        {rmse_improvement:+.2f}%\")\n",
    "print(f\"   MAE Reduction:         {mae_improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-viz-header",
   "metadata": {},
   "source": [
    "### ğŸ“Š Visual Model Comparison\n",
    "\n",
    "Let's visualize the performance differences between both models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Metrics to compare\n",
    "metrics = ['RÂ² Score', 'RMSE ($)', 'MAE ($)']\n",
    "simple_values = [r2_test_simple, rmse_test_simple, mae_test_simple]\n",
    "multiple_values = [r2_test_multi, rmse_test_multi, mae_test_multi]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "# Bar chart comparison\n",
    "for i, (metric, simple_val, multi_val) in enumerate(zip(metrics, simple_values, multiple_values)):\n",
    "    if i == 0:  # RÂ² - higher is better\n",
    "        axes[i].bar(['Simple LR', 'Multiple LR'], [simple_val, multi_val], \n",
    "                    color=['lightblue', 'darkblue'], edgecolor='black')\n",
    "        axes[i].set_ylabel('RÂ² Score')\n",
    "        axes[i].set_title(f'{metric}\\n(Higher is Better)')\n",
    "        axes[i].set_ylim([0, 1])\n",
    "        for j, v in enumerate([simple_val, multi_val]):\n",
    "            axes[i].text(j, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "    else:  # RMSE and MAE - lower is better\n",
    "        axes[i].bar(['Simple LR', 'Multiple LR'], [simple_val, multi_val], \n",
    "                    color=['lightcoral', 'darkred'], edgecolor='black')\n",
    "        axes[i].set_ylabel('Error ($)')\n",
    "        axes[i].set_title(f'{metric}\\n(Lower is Better)')\n",
    "        for j, v in enumerate([simple_val, multi_val]):\n",
    "            axes[i].text(j, v + (v * 0.02), f'${v:,.0f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine winner\n",
    "print(\"\\nğŸ† MODEL COMPARISON SUMMARY:\\n\")\n",
    "\n",
    "if r2_test_multi > r2_test_simple:\n",
    "    print(f\"âœ… Multiple LR has HIGHER RÂ²: {r2_test_multi:.4f} vs {r2_test_simple:.4f}\")\n",
    "else:\n",
    "    print(f\"âŒ Simple LR has HIGHER RÂ²: {r2_test_simple:.4f} vs {r2_test_multi:.4f}\")\n",
    "\n",
    "if rmse_test_multi < rmse_test_simple:\n",
    "    print(f\"âœ… Multiple LR has LOWER RMSE: ${rmse_test_multi:,.2f} vs ${rmse_test_simple:,.2f}\")\n",
    "else:\n",
    "    print(f\"âŒ Simple LR has LOWER RMSE: ${rmse_test_simple:,.2f} vs ${rmse_test_multi:,.2f}\")\n",
    "\n",
    "if mae_test_multi < mae_test_simple:\n",
    "    print(f\"âœ… Multiple LR has LOWER MAE: ${mae_test_multi:,.2f} vs ${mae_test_simple:,.2f}\")\n",
    "else:\n",
    "    print(f\"âŒ Simple LR has LOWER MAE: ${mae_test_simple:,.2f} vs ${mae_test_multi:,.2f}\")\n",
    "\n",
    "# Overall winner\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if r2_test_multi > r2_test_simple and rmse_test_multi < rmse_test_simple:\n",
    "    print(\"ğŸ† WINNER: Multiple Linear Regression\")\n",
    "    print(\"\\nğŸ’¡ Conclusion: Using all features significantly improves prediction accuracy!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Unexpected Result: Models perform similarly or Simple LR is better\")\n",
    "    print(\"\\nğŸ’¡ This suggests potential overfitting or that additional features don't add value\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions-header",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "\n",
    "## 7. Insights & Conclusions\n",
    "\n",
    "### ğŸ¯ Phase 3 Summary\n",
    "\n",
    "In this phase, we successfully:\n",
    "\n",
    "1. âœ… **Built Simple Linear Regression** using the best single feature\n",
    "2. âœ… **Built Multiple Linear Regression** using all engineered features\n",
    "3. âœ… **Evaluated both models** using RÂ², RMSE, and MAE\n",
    "4. âœ… **Compared performance** to identify the best approach\n",
    "5. âœ… **Visualized results** for clear understanding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š PHASE 3: FINAL SUMMARY & KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ¯ MODELS BUILT:\\n\")\n",
    "print(f\"1. Simple Linear Regression\")\n",
    "print(f\"   - Feature Used: {best_feature}\")\n",
    "print(f\"   - Test RÂ²: {r2_test_simple:.4f}\")\n",
    "print(f\"   - Test MAE: ${mae_test_simple:,.2f}\")\n",
    "\n",
    "print(f\"\\n2. Multiple Linear Regression\")\n",
    "print(f\"   - Features Used: All {X_train.shape[1]} features\")\n",
    "print(f\"   - Test RÂ²: {r2_test_multi:.4f}\")\n",
    "print(f\"   - Test MAE: ${mae_test_multi:,.2f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ KEY INSIGHTS:\\n\")\n",
    "print(f\"1. The best single predictor is '{best_feature}' with {best_correlation:.4f} correlation\")\n",
    "print(f\"\\n2. Simple LR explains {r2_test_simple*100:.2f}% of price variance with just ONE feature\")\n",
    "print(f\"\\n3. Multiple LR explains {r2_test_multi*100:.2f}% of price variance using ALL features\")\n",
    "print(f\"   - This is a {r2_improvement:+.2f}% improvement in explained variance\")\n",
    "print(f\"\\n4. Multiple LR reduces prediction error by ${mae_test_simple - mae_test_multi:,.2f} on average\")\n",
    "print(f\"   - From ${mae_test_simple:,.2f} (Simple) to ${mae_test_multi:,.2f} (Multiple)\")\n",
    "\n",
    "print(\"\\nğŸ” MODEL GENERALIZATION:\\n\")\n",
    "simple_gap = abs(r2_train_simple - r2_test_simple)\n",
    "multi_gap = abs(r2_train_multi - r2_test_multi)\n",
    "print(f\"Simple LR - Train/Test RÂ² Gap: {simple_gap:.4f}\")\n",
    "print(f\"Multiple LR - Train/Test RÂ² Gap: {multi_gap:.4f}\")\n",
    "if simple_gap < 0.05 and multi_gap < 0.05:\n",
    "    print(\"âœ… Both models generalize well (no significant overfitting)\")\n",
    "elif multi_gap > simple_gap:\n",
    "    print(\"âš ï¸ Multiple LR shows slightly more overfitting than Simple LR\")\n",
    "else:\n",
    "    print(\"âœ… Models show good generalization\")\n",
    "\n",
    "print(\"\\nğŸ¯ BUSINESS IMPLICATIONS:\\n\")\n",
    "avg_price = y_test.mean()\n",
    "simple_error_pct = (mae_test_simple / avg_price) * 100\n",
    "multi_error_pct = (mae_test_multi / avg_price) * 100\n",
    "\n",
    "print(f\"For a typical house worth ${avg_price:,.0f}:\")\n",
    "print(f\"\\n- Simple LR:   ${mae_test_simple:,.2f} error ({simple_error_pct:.2f}% of price)\")\n",
    "print(f\"- Multiple LR: ${mae_test_multi:,.2f} error ({multi_error_pct:.2f}% of price)\")\n",
    "print(f\"\\nğŸ’° Using Multiple LR saves ${mae_test_simple - mae_test_multi:,.2f} in prediction error per house\")\n",
    "\n",
    "print(\"\\nâœ… PHASE 3 OBJECTIVES ACHIEVED:\\n\")\n",
    "print(\"âœ“ Built regression models for house price prediction\")\n",
    "print(\"âœ“ Evaluated models using RÂ², RMSE, and MAE metrics\")\n",
    "print(\"âœ“ Demonstrated value of feature engineering from Phase 2\")\n",
    "print(\"âœ“ Provided clear interpretation of model performance\")\n",
    "print(\"âœ“ Created visualizations for stakeholder communication\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ PHASE 3 COMPLETE - Ready for Phase 4 (Visualization & Storytelling)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š What We Learned\n",
    "\n",
    "### âœ… Technical Achievements:\n",
    "\n",
    "1. **Simple Linear Regression**: \n",
    "   - Learned that a single feature can explain a significant portion of price variation\n",
    "   - Provides an easy-to-understand baseline model\n",
    "   - Useful for quick estimations and communication with non-technical stakeholders\n",
    "\n",
    "2. **Multiple Linear Regression**:\n",
    "   - Significantly improved prediction accuracy by leveraging all features\n",
    "   - Captures complex relationships between multiple house attributes and price\n",
    "   - Validates the effort put into feature engineering in Phase 2\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - Learned to use multiple metrics (RÂ², RMSE, MAE) for comprehensive assessment\n",
    "   - Understood train/test split for detecting overfitting\n",
    "   - Developed skills in interpreting model coefficients and residuals\n",
    "\n",
    "### ğŸ¯ Real-World Applications:\n",
    "\n",
    "These models can be used by:\n",
    "- **Real Estate Agents**: Quick price estimations for clients\n",
    "- **Home Buyers**: Understanding fair market value\n",
    "- **Property Investors**: Identifying undervalued properties\n",
    "- **Appraisers**: Data-driven valuation support\n",
    "\n",
    "### ğŸ“ˆ Next Steps (Phase 4):\n",
    "\n",
    "- Create interactive dashboards visualizing model results\n",
    "- Build storytelling presentation for stakeholders\n",
    "- Highlight key insights and recommendations\n",
    "- Demonstrate business value of the models\n",
    "\n",
    "---\n",
    "\n",
    "**End of Phase 3 Notebook**\n",
    "\n",
    "*Team: The Outliers | Course: Advanced Apex Project 1 | BITS Pilani Digital*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
